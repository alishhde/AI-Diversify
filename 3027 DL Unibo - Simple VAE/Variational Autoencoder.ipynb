{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4630fc0f",
   "metadata": {
    "id": "d_DKvNpcIeVu",
    "papermill": {
     "duration": 0.008794,
     "end_time": "2024-05-20T18:53:16.728109",
     "exception": false,
     "start_time": "2024-05-20T18:53:16.719315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Variational Autoencoder\n",
    "In this notebook we shall present a simple conditional Variational Autoencoder, trained on MNIST. VAE is a type of generative model that combines principles from variational inference and autoencoders. It is used for generating new data samples that are similar to a given dataset. VAEs are particularly useful in scenarios such as image generation, data compression, and representation learning. \n",
    "\n",
    "## Components:\n",
    "\n",
    "1. **Encoder**:\n",
    "   - The encoder is a neural network that takes an input data sample (e.g., an image) and maps it to a latent space. Instead of directly outputting a single point in the latent space, the encoder outputs parameters of a probability distribution (typically a Gaussian distribution), namely the mean ($\\mu$) and the standard deviation ($\\sigma$).\n",
    "\n",
    "\n",
    "2. **Latent Space**:\n",
    "   - The latent space is a lower-dimensional representation of the input data. It captures the underlying factors of variation in the data. Each point in the latent space can be sampled to generate new data points.\n",
    "\n",
    "\n",
    "3. **Decoder**:\n",
    "   - The decoder is another neural network that takes a point from the latent space and maps it back to the data space. It reconstructs the original data sample from the latent representation.\n",
    "\n",
    "\n",
    "## Working of VAE:\n",
    "\n",
    "1. **Encoding**:\n",
    "   - An input sample $x$ is passed through the encoder, which outputs the parameters $\\mu$ and $\\sigma$ of the latent distribution.\n",
    "   - Instead of using a single deterministic point, a random sample $z$ is drawn from the Gaussian distribution defined by $\\mu$ and $\\sigma$. This sampling process is essential for the generative capabilities of the VAE.\n",
    "\n",
    "2. **Reparameterization Trick**:\n",
    "   - To enable backpropagation through the sampling process, the reparameterization trick is used. It involves expressing the sampled latent vector $z$ as:\n",
    "     $\n",
    "     z = \\mu + \\sigma \\odot \\epsilon\n",
    "     $\n",
    "     where $\\epsilon$ is a random noise vector sampled from a standard normal distribution, and $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "3. **Decoding**:\n",
    "   - The sampled latent vector $z$ is passed through the decoder to reconstruct the original data sample $\\hat{x}$.\n",
    "\n",
    "4. **Loss Function**:\n",
    "   - The VAE is trained using a loss function that has two components:\n",
    "     - **Reconstruction Loss**: Measures how well the reconstructed data $\\hat{x}$ matches the original input $x$. This is typically the Mean Squared Error (MSE) for continuous data or Binary Cross-Entropy (BCE) for binary data.\n",
    "     - **KL Divergence Loss**: Measures the divergence between the learned latent distribution $q(z|x)$ and the prior distribution $p(z)$, which is usually a standard normal distribution. This encourages the learned latent space to be close to the prior distribution.\n",
    "     $$\n",
    "     \\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{\\text{KL}}(q(z|x) \\parallel p(z))\n",
    "     $$\n",
    "\n",
    "\n",
    "## Notes\n",
    "- **Generative Model**: VAEs are used to generate new data samples that resemble the training data.\n",
    "- **Latent Space**: They learn a continuous, lower-dimensional latent space representing the data.\n",
    "- **Probabilistic Encoder**: The encoder outputs a distribution over the latent space, rather than a single point.\n",
    "- **Training Objective**: The training optimizes both the reconstruction of the input data and the regularization of the latent space distribution.\n",
    "\n",
    "VAEs have found applications in various fields, including image generation, text generation, and anomaly detection, due to their ability to learn meaningful latent representations and generate diverse samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696ede8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:16.747586Z",
     "iopub.status.busy": "2024-05-20T18:53:16.746770Z",
     "iopub.status.idle": "2024-05-20T18:53:33.408394Z",
     "shell.execute_reply": "2024-05-20T18:53:33.406737Z"
    },
    "id": "V8_HUth4IW1g",
    "papermill": {
     "duration": 16.675184,
     "end_time": "2024-05-20T18:53:33.411625",
     "exception": false,
     "start_time": "2024-05-20T18:53:16.736441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 18:53:19.152468: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-20 18:53:19.152642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-20 18:53:19.310126: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import utils\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64bc711",
   "metadata": {
    "id": "bKj7GMTI7eK2",
    "papermill": {
     "duration": 0.008004,
     "end_time": "2024-05-20T18:53:33.428104",
     "exception": false,
     "start_time": "2024-05-20T18:53:33.420100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We want to use the variational autoencoder technique to generate digits similar to those in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c01ce20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:33.447289Z",
     "iopub.status.busy": "2024-05-20T18:53:33.446507Z",
     "iopub.status.idle": "2024-05-20T18:53:34.207635Z",
     "shell.execute_reply": "2024-05-20T18:53:34.206071Z"
    },
    "id": "_WG8B8RJlAaW",
    "papermill": {
     "duration": 0.774173,
     "end_time": "2024-05-20T18:53:34.210669",
     "exception": false,
     "start_time": "2024-05-20T18:53:33.436496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# train the VAE on MNIST digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))  # Result: (num_samples, 784)\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "y_train = utils.to_categorical(y_train)  # Converting integer labels to One-hot Encodding\n",
    "y_test = utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a66c6d",
   "metadata": {
    "id": "OxLa37ZnKIUV",
    "papermill": {
     "duration": 0.0086,
     "end_time": "2024-05-20T18:53:34.227841",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.219241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Model\n",
    "Sampling function is a key part of implementing a VAE. It is used to sample latent vectors from the learned latent space distribution during the encoding process. In a VAE, instead of encoding an input to a fixed point in the latent space, we encode it to a distribution (usually Gaussian). The sampling function performs the crucial step of sampling from this distribution to obtain a latent vector that will be passed to the decoder for reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53d89c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:34.247111Z",
     "iopub.status.busy": "2024-05-20T18:53:34.246639Z",
     "iopub.status.idle": "2024-05-20T18:53:34.253437Z",
     "shell.execute_reply": "2024-05-20T18:53:34.252177Z"
    },
    "id": "t1hiFRyHJbKf",
    "papermill": {
     "duration": 0.019195,
     "end_time": "2024-05-20T18:53:34.255843",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.236648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), \n",
    "                              mean=0.,\n",
    "                              stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e71289",
   "metadata": {
    "papermill": {
     "duration": 0.007952,
     "end_time": "2024-05-20T18:53:34.272105",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.264153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Code Explanation \n",
    "- The function takes a single argument args, which is a tuple containing `z_mean` and `z_log_var`. These are the outputs of the encoder network.\n",
    "- `z_mean` represents the mean of the latent Gaussian distribution.\n",
    "- `z_log_var` represents the logarithm of the variance of the latent Gaussian distribution.\n",
    "- `epsilon` is a random noise vector sampled from a standard normal distribution with mean 0 and standard deviation 1.\n",
    "    - `K.random_normal` is a Keras backend function to generate random normal values. The shape of `epsilon` matches the batch size and the dimensionality of the latent space (`latent_dim`).\n",
    "- The return line implements the `reparameterization` trick, which allows the gradients to backpropagate through the sampling process.\n",
    "    - `K.exp(z_log_var / 2)` computes the standard deviation from the log variance.\n",
    "    - The latent vector `z` is computed by shifting the mean (`z_mean`) and scaling by the standard deviation (`K.exp(z_log_var / 2)`) with the random noise `epsilon`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8389220",
   "metadata": {
    "id": "cwm4jR5E8in4",
    "papermill": {
     "duration": 0.007991,
     "end_time": "2024-05-20T18:53:34.288309",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.280318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Main dimensions for the model (a simple stack of dense layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995e847f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:34.307261Z",
     "iopub.status.busy": "2024-05-20T18:53:34.306399Z",
     "iopub.status.idle": "2024-05-20T18:53:34.311306Z",
     "shell.execute_reply": "2024-05-20T18:53:34.310370Z"
    },
    "id": "VT9baosbKW8M",
    "papermill": {
     "duration": 0.017107,
     "end_time": "2024-05-20T18:53:34.313565",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.296458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "latent_dim = 16\n",
    "intermediate_dim_1 = 128\n",
    "intermediate_dim_2 = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74b07b",
   "metadata": {
    "id": "533QeAx09JGh",
    "papermill": {
     "duration": 0.007908,
     "end_time": "2024-05-20T18:53:34.330966",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.323058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1. Encoder\n",
    "We start with the encoder. It takes two inputs: the image and the category. It returns the latent encoding `z_mean` and a `(log-)variance` for each latent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a819c003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:34.350212Z",
     "iopub.status.busy": "2024-05-20T18:53:34.349337Z",
     "iopub.status.idle": "2024-05-20T18:53:34.457521Z",
     "shell.execute_reply": "2024-05-20T18:53:34.456462Z"
    },
    "id": "9atMqlZUJ_dD",
    "papermill": {
     "duration": 0.120895,
     "end_time": "2024-05-20T18:53:34.460112",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.339217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "x = layers.Input(shape=(input_dim, ))  # Input shape equals to 28*28\n",
    "\n",
    "# Dense Layers\n",
    "h1 = layers.Dense(intermediate_dim_1, activation='swish')(x)\n",
    "h2 = layers.Dense(intermediate_dim_2, activation='swish')(h1)\n",
    "\n",
    "# Output Layers\n",
    "z_mean = layers.Dense(latent_dim)(h2)\n",
    "z_log_var = layers.Dense(latent_dim)(h2)\n",
    "\n",
    "encoder = Model(x, [z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d25450",
   "metadata": {
    "id": "wuIsUslmBKxj",
    "papermill": {
     "duration": 0.008807,
     "end_time": "2024-05-20T18:53:34.477309",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.468502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2. Decoder\n",
    "Now we define the decoder. It takes in input a vector in the latent space, and it returns the image of a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54857485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:34.497279Z",
     "iopub.status.busy": "2024-05-20T18:53:34.496385Z",
     "iopub.status.idle": "2024-05-20T18:53:34.540014Z",
     "shell.execute_reply": "2024-05-20T18:53:34.539027Z"
    },
    "id": "0NX6uciuBKJw",
    "papermill": {
     "duration": 0.056777,
     "end_time": "2024-05-20T18:53:34.542677",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.485900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "y = layers.Input(shape=(latent_dim, ))\n",
    "\n",
    "# Dense Layers\n",
    "dec_mid_1 = layers.Dense(intermediate_dim_2, activation='swish')(y)\n",
    "dec_mid_2 = layers.Dense(intermediate_dim_1, activation='swish')(dec_mid_1)\n",
    "\n",
    "# Output Layer\n",
    "x_hat = layers.Dense(input_dim, activation='sigmoid')(dec_mid_2)\n",
    "\n",
    "decoder = Model(inputs=y, outputs=[x_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32496261",
   "metadata": {
    "id": "CL94ost69tn_",
    "papermill": {
     "duration": 0.008479,
     "end_time": "2024-05-20T18:53:34.559754",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.551275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3. Building VAE\n",
    "We build the VAE by composing the encoder and the decoder. However, between them we need to insert the sampling operation. In order to wrap the sampling function into a layer we use a special layer called \"lambda\". \n",
    "\n",
    "Sample code is as follow, for our case, due to unexpected error, we have implemented our own Training class with respect to the following sample code:\n",
    "```Python\n",
    "# Input Layer\n",
    "x = layers.Input(shape=(input_dim, ))\n",
    "\n",
    "# Encoding\n",
    "z_mean, z_log_var = encoder(x)\n",
    "\n",
    "# Inserting Sampling Operation between Encoder-Decoder\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim, ))([z_mean, z_log_var])\n",
    "\n",
    "# Decoding\n",
    "x_hat = decoder(z)\n",
    "\n",
    "# Final Model is composed by the ...\n",
    "vae = Model(x, x_hat)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f49535d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:34.580959Z",
     "iopub.status.busy": "2024-05-20T18:53:34.580063Z",
     "iopub.status.idle": "2024-05-20T18:53:34.595493Z",
     "shell.execute_reply": "2024-05-20T18:53:34.594093Z"
    },
    "id": "RrqVE-8gLVkP",
    "papermill": {
     "duration": 0.029414,
     "end_time": "2024-05-20T18:53:34.598534",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.569120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(data)\n",
    "            z = sampling([z_mean, z_log_var])\n",
    "            x_hat = self.decoder(z)\n",
    "            loss = vae_loss(data, x_hat, z_mean, z_log_var)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_var = self.encoder(data)\n",
    "        z = sampling([z_mean, z_log_var])\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = vae_loss(data, x_hat, z_mean, z_log_var)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "# Instantiate and compile the VAE model\n",
    "vae = VAE(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df8554",
   "metadata": {
    "id": "HFLYIrj8Ds9l",
    "papermill": {
     "duration": 0.008206,
     "end_time": "2024-05-20T18:53:34.615243",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.607037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The VAE loss function is just the sum between the `reconstruction error` (mse or bce) and the `KL-divergence`, acting as a regularizer of the latent space. In other words, The loss function combines two components: the `reconstruction loss` and the `Kullback-Leibler (KL) divergence`. This combination ensures that the VAE learns to generate data that is similar to the input data while maintaining a well-formed latent space. \n",
    "\n",
    "- Reconstruction Loss:\n",
    "    - Purpose: Measures how well the VAE can reconstruct the input data 𝑥 from the latent representation.\n",
    "    \n",
    "    \n",
    "- KL-Divergence Loss:\n",
    "    - Purpose: Regularizes the latent space to ensure that the learned distribution is close to a standard normal distribution 𝑁(0, 𝐼). This regularization prevents overfitting and ensures smooth interpolation in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487ab9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:34.634780Z",
     "iopub.status.busy": "2024-05-20T18:53:34.634317Z",
     "iopub.status.idle": "2024-05-20T18:53:34.678353Z",
     "shell.execute_reply": "2024-05-20T18:53:34.676998Z"
    },
    "papermill": {
     "duration": 0.05744,
     "end_time": "2024-05-20T18:53:34.680966",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.623526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vae\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vae\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">105,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">105,904</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_1 (\u001b[38;5;33mFunctional\u001b[0m)       │ ?                      │       \u001b[38;5;34m105,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_3 (\u001b[38;5;33mFunctional\u001b[0m)       │ ?                      │       \u001b[38;5;34m105,904\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,568</span> (826.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m211,568\u001b[0m (826.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,568</span> (826.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m211,568\u001b[0m (826.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = 1.0 # balancing factor\n",
    "\n",
    "# Custom loss function\n",
    "def vae_loss(x, x_hat, z_mean, z_log_var):\n",
    "    rec_loss = input_dim * metrics.binary_crossentropy(x, x_hat)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return K.mean(rec_loss + kl_loss)\n",
    "\n",
    "# Compile the model with the custom loss\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "# Summary of the model (optional)\n",
    "vae.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a02730",
   "metadata": {
    "papermill": {
     "duration": 0.009051,
     "end_time": "2024-05-20T18:53:34.699362",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.690311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Code Explanation\n",
    "- `beta` Controls the trade-off between the reconstruction loss and the KL divergence loss.\n",
    "    - By adjusting 𝛽, you can control the importance of the KL divergence regularization relative to the reconstruction loss. Setting 𝛽=1 gives equal weight to both losses. Adjusting 𝛽 can help in balancing between generating accurate reconstructions and ensuring a well-behaved latent space.\n",
    "- `metrics.binary_crossentropy(x, x_hat)` computes the binary cross-entropy between the original input 𝑥 and the reconstructed output 𝑥^.\n",
    "- `1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)` computes the element-wise KL divergence for each latent variable.\n",
    "- `K.sum(..., axis=-1)` sums the KL divergence terms over the latent dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b426e404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:34.720542Z",
     "iopub.status.busy": "2024-05-20T18:53:34.720039Z",
     "iopub.status.idle": "2024-05-20T18:53:34.726142Z",
     "shell.execute_reply": "2024-05-20T18:53:34.724773Z"
    },
    "id": "bBZvcj_VnRjE",
    "papermill": {
     "duration": 0.019377,
     "end_time": "2024-05-20T18:53:34.728638",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.709261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ceffcd",
   "metadata": {
    "id": "2I2N8blv_kT2",
    "papermill": {
     "duration": 0.008936,
     "end_time": "2024-05-20T18:53:34.746755",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.737819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Train for a sufficient amount of epochs. Generation is a more complex task than classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df518c26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:53:34.767566Z",
     "iopub.status.busy": "2024-05-20T18:53:34.767040Z",
     "iopub.status.idle": "2024-05-20T18:59:25.153993Z",
     "shell.execute_reply": "2024-05-20T18:59:25.152483Z"
    },
    "id": "VgjJ9-sElLFe",
    "outputId": "906fe5aa-c002-4830-ab28-d27ca9768cc7",
    "papermill": {
     "duration": 350.401373,
     "end_time": "2024-05-20T18:59:25.157381",
     "exception": false,
     "start_time": "2024-05-20T18:53:34.756008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 784)\n",
      "Validation data shape: (10000, 784)\n",
      "Epoch 1/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 192.7300 - val_loss: 0.0000e+00\n",
      "Epoch 2/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 146.5652 - val_loss: 0.0000e+00\n",
      "Epoch 3/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 136.1186 - val_loss: 0.0000e+00\n",
      "Epoch 4/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 129.7634 - val_loss: 0.0000e+00\n",
      "Epoch 5/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 125.2633 - val_loss: 0.0000e+00\n",
      "Epoch 6/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 121.8837 - val_loss: 0.0000e+00\n",
      "Epoch 7/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 118.9654 - val_loss: 0.0000e+00\n",
      "Epoch 8/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 116.9854 - val_loss: 0.0000e+00\n",
      "Epoch 9/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 115.6083 - val_loss: 0.0000e+00\n",
      "Epoch 10/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 114.5964 - val_loss: 0.0000e+00\n",
      "Epoch 11/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 113.6767 - val_loss: 0.0000e+00\n",
      "Epoch 12/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 112.9794 - val_loss: 0.0000e+00\n",
      "Epoch 13/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 112.3440 - val_loss: 0.0000e+00\n",
      "Epoch 14/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 111.8720 - val_loss: 0.0000e+00\n",
      "Epoch 15/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 111.4497 - val_loss: 0.0000e+00\n",
      "Epoch 16/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 111.0490 - val_loss: 0.0000e+00\n",
      "Epoch 17/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 110.6481 - val_loss: 0.0000e+00\n",
      "Epoch 18/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 110.3611 - val_loss: 0.0000e+00\n",
      "Epoch 19/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 110.0563 - val_loss: 0.0000e+00\n",
      "Epoch 20/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 109.7970 - val_loss: 0.0000e+00\n",
      "Epoch 21/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 109.5832 - val_loss: 0.0000e+00\n",
      "Epoch 22/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 109.3244 - val_loss: 0.0000e+00\n",
      "Epoch 23/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 109.1084 - val_loss: 0.0000e+00\n",
      "Epoch 24/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 108.9504 - val_loss: 0.0000e+00\n",
      "Epoch 25/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 108.7798 - val_loss: 0.0000e+00\n",
      "Epoch 26/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 108.6264 - val_loss: 0.0000e+00\n",
      "Epoch 27/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 108.4137 - val_loss: 0.0000e+00\n",
      "Epoch 28/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 108.3158 - val_loss: 0.0000e+00\n",
      "Epoch 29/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 108.1812 - val_loss: 0.0000e+00\n",
      "Epoch 30/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 108.0111 - val_loss: 0.0000e+00\n",
      "Epoch 31/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 107.8687 - val_loss: 0.0000e+00\n",
      "Epoch 32/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 107.7434 - val_loss: 0.0000e+00\n",
      "Epoch 33/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 107.6552 - val_loss: 0.0000e+00\n",
      "Epoch 34/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 107.5353 - val_loss: 0.0000e+00\n",
      "Epoch 35/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 107.4298 - val_loss: 0.0000e+00\n",
      "Epoch 36/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 107.3189 - val_loss: 0.0000e+00\n",
      "Epoch 37/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 107.2271 - val_loss: 0.0000e+00\n",
      "Epoch 38/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 107.1231 - val_loss: 0.0000e+00\n",
      "Epoch 39/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 107.0138 - val_loss: 0.0000e+00\n",
      "Epoch 40/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 106.9433 - val_loss: 0.0000e+00\n",
      "Epoch 41/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 106.8479 - val_loss: 0.0000e+00\n",
      "Epoch 42/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 106.7465 - val_loss: 0.0000e+00\n",
      "Epoch 43/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 106.6802 - val_loss: 0.0000e+00\n",
      "Epoch 44/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 106.6086 - val_loss: 0.0000e+00\n",
      "Epoch 45/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 106.5284 - val_loss: 0.0000e+00\n",
      "Epoch 46/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 106.4678 - val_loss: 0.0000e+00\n",
      "Epoch 47/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 106.3653 - val_loss: 0.0000e+00\n",
      "Epoch 48/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 106.3216 - val_loss: 0.0000e+00\n",
      "Epoch 49/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 106.2229 - val_loss: 0.0000e+00\n",
      "Epoch 50/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 106.1619 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Print shapes for debugging\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Validation data shape:\", x_test.shape)\n",
    "\n",
    "vae.fit(x_train, shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None))\n",
    "\n",
    "vae.save_weights(\"vae.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f71ff1",
   "metadata": {
    "id": "OgS9EBeAGVBM",
    "papermill": {
     "duration": 0.518741,
     "end_time": "2024-05-20T18:59:26.279924",
     "exception": false,
     "start_time": "2024-05-20T18:59:25.761183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us plot some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc4fbb95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:59:27.273769Z",
     "iopub.status.busy": "2024-05-20T18:59:27.273339Z",
     "iopub.status.idle": "2024-05-20T18:59:27.281597Z",
     "shell.execute_reply": "2024-05-20T18:59:27.280271Z"
    },
    "id": "QhEuyvLqmg9Z",
    "papermill": {
     "duration": 0.505901,
     "end_time": "2024-05-20T18:59:27.284259",
     "exception": false,
     "start_time": "2024-05-20T18:59:26.778358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot(images):\n",
    "    n = images.shape[0]\n",
    "    plt.figure(figsize=(2*n, 2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(images[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee37e33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:59:28.245778Z",
     "iopub.status.busy": "2024-05-20T18:59:28.245372Z",
     "iopub.status.idle": "2024-05-20T18:59:29.289067Z",
     "shell.execute_reply": "2024-05-20T18:59:29.287439Z"
    },
    "id": "kts9354Pmq9Y",
    "outputId": "7c7ce849-1593-414c-ebbe-d8b089fc8aa8",
    "papermill": {
     "duration": 1.531236,
     "end_time": "2024-05-20T18:59:29.292000",
     "exception": false,
     "start_time": "2024-05-20T18:59:27.760764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_sample = np.random.normal(size=(10, latent_dim))  # Creating 10 samples - latent_dim is dimensionality of the latent space\n",
    "generated = decoder.predict(z_sample)  # generates new data from the random latent samples.\n",
    "plot(generated)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VAE.ipynb",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 379.622283,
   "end_time": "2024-05-20T18:59:32.855142",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-20T18:53:13.232859",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
