{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a480831e",
   "metadata": {
    "papermill": {
     "duration": 0.007738,
     "end_time": "2024-05-06T08:16:01.808278",
     "exception": false,
     "start_time": "2024-05-06T08:16:01.800540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction to BERT\n",
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a groundbreaking model in the field of natural language processing (NLP) developed by researchers at Google. Introduced in a `2018` paper by `Jacob Devlin` and his colleagues, BERT's key innovation lies in its ability to train language representations in a deeply bidirectional way.\n",
    "\n",
    "### Key Features of BERT:\n",
    "\n",
    "1. **Bidirectional Training**: Traditional language models were typically trained to predict the next word in a sequence, meaning they were unidirectional. This limits the model's ability to learn context from both the left and the right side of a word. BERT, however, uses the Transformer architecture, which allows it to consider the full context of a word by looking at words that come before and after it—essentially, it reads the entire sentence, both left and right of every word, at once.\n",
    "\n",
    "2. **Transformer Architecture**: BERT is based on the Transformer model, which uses an attention mechanism to weigh the influence of different words on each other’s representation. Unlike directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer reads the whole sequence of words at once. This is part of what allows BERT to understand the context more effectively.\n",
    "\n",
    "3. **Masked Language Model (MLM)**: During training, random words in a sentence are replaced with a \"[MASK]\" token, and the model is trained to predict the original word based on the context provided by the other non-masked words in the sequence. This training process is called `Masked LM` and allows BERT to flexibly understand and predict words based on their context.\n",
    "\n",
    "4. **Next Sentence Prediction (NSP)**: In addition to predicting masked words, BERT is also trained to predict whether a sentence logically follows another. This capability is trained using pairs of sentences, which helps the model understand the relationship between consecutive sentences, aiding in tasks like question answering and language inference.\n",
    "\n",
    "### Applications of BERT:\n",
    "\n",
    "BERT has been revolutionary in the field of NLP and has led to improvements in many different language understanding tasks, including:\n",
    "- **Text Classification**: BERT can be fine-tuned for various classification tasks like sentiment analysis or spam detection.\n",
    "- **Question Answering**: BERT excels in extracting answers from texts given a question.\n",
    "- **Named Entity Recognition (NER)**: Identifying names of people, organizations, locations, etc., in text.\n",
    "- **Language Translation**: Although not its initial purpose, BERT can be adapted for translation tasks.\n",
    "\n",
    "BERT set a new standard for NLP models and has inspired a host of variants and improvements, such as RoBERTa, ALBERT, and DistilBERT, each aiming to optimize various aspects of the original BERT model, like training speed, model size, and accuracy.\n",
    "\n",
    "For implementing BERT in a project, libraries like Hugging Face's Transformers provide pre-trained models which can be fine-tuned on specific datasets relatively easily. This accessibility has made advanced NLP capabilities available to a wide range of developers and researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b2d1d",
   "metadata": {
    "papermill": {
     "duration": 0.007034,
     "end_time": "2024-05-06T08:16:01.823685",
     "exception": false,
     "start_time": "2024-05-06T08:16:01.816651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. BERT Variations\n",
    "BERT, since its introduction, has inspired a number of variants that aim to improve upon the original model in various ways, such as efficiency, performance on specific tasks, or adaptability to different languages or smaller datasets. Here's an overview of some notable versions of BERT:\n",
    "\n",
    "1. **RoBERTa (Robustly Optimized BERT Approach)**:\n",
    "   Developed by Facebook AI, RoBERTa modifies key hyperparameters in BERT and removes the Next Sentence Prediction (NSP) objective used during pre-training. It is trained with much larger mini-batches and learning rates, and on more data for a longer period of time compared to BERT. These changes help RoBERTa to outperform BERT on many NLP benchmarks.\n",
    "\n",
    "2. **DistilBERT**:\n",
    "   DistilBERT is a smaller, faster, cheaper, and lighter version of BERT developed by Hugging Face. It is designed to maintain most of the performance of BERT, while reducing the model size by about 40%. DistilBERT is achieved by a process called distillation, which involves training a smaller model (the \"student\") to reproduce the behavior of a larger pre-trained model (the \"teacher\").\n",
    "\n",
    "3. **ALBERT (A Lite BERT)**:\n",
    "   ALBERT is designed to reduce the memory consumption and increase the training speed of BERT. It introduces two main innovations: `factorized embedding parameterization` and `cross-layer parameter sharing`. The embedding parameterization separates the size of the hidden layers from the size of vocabulary embeddings, reducing the number of parameters. Cross-layer parameter sharing helps in reducing the model size further by sharing parameters across the different layers of the model.\n",
    "\n",
    "4. **TinyBERT**:\n",
    "   TinyBERT goes further than DistilBERT in terms of model size reduction. It is specifically designed to be small enough for deployment on resource-restricted devices like mobile phones. TinyBERT involves a two-stage training process: transformer distillation at pre-training and task-specific distillation.\n",
    "\n",
    "5. **BERT-large, BERT-base, and Other Sizes**:\n",
    "   The original BERT model comes in two sizes – `BERT-base` and `BERT-large`. BERT-base has 110 million parameters consisting of `12 layers`, while BERT-large has 340 million parameters with `24 layers`. These different sizes offer trade-offs between performance and computational efficiency.\n",
    "\n",
    "6. **Multilingual BERT (mBERT)**:\n",
    "   Released by Google, this version of BERT is trained on Wikipedia articles in 104 languages. It facilitates tasks that involve multiple languages and is particularly valuable for applications requiring cross-linguistic transfer learning.\n",
    "\n",
    "7. **BERT for Sequence-to-Sequence Applications (BART and T5)**:\n",
    "   While not direct versions of BERT, BART and T5 are notable extensions. BART is designed to pre-train sequence-to-sequence models by corrupting text with an arbitrary noising function and learning a model to reconstruct the original text. T5 (Text-to-Text Transfer Transformer) generalizes the idea further by framing all NLP tasks as a text-to-text problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6103760",
   "metadata": {
    "papermill": {
     "duration": 0.006865,
     "end_time": "2024-05-06T08:16:01.837500",
     "exception": false,
     "start_time": "2024-05-06T08:16:01.830635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. BERT's Key Architectural Parameters\n",
    "In the various versions and configurations of BERT, the terms \"L\", \"H\", and \"A\" represent key architectural parameters that define the model's structure and capacity.\n",
    "\n",
    "1. **`L` - Number of Layers (also referred to as Transformer Blocks):**\n",
    "   - \"L\" stands for the number of layers, or depth, of the network. Each layer is a Transformer block that contains mechanisms for self-attention and feed-forward connections. In BERT architectures, more layers generally mean a greater ability to capture complex features and relationships in the data, but at the cost of increased computational complexity and potential difficulties in training (such as handling vanishing gradients).\n",
    "   - For example, BERT-base has 12 layers, while BERT-large has 24 layers.\n",
    "\n",
    "2. **`H` - Size of Hidden Layers:**\n",
    "   - \"H\" denotes the size of the hidden layers, which is the dimensionality of the hidden states in the model. This parameter is a critical factor in determining the model's capacity to learn; higher values allow the model to learn more detailed features with more nuanced representations of the input data, but also require more computation and memory.\n",
    "   - In BERT-base, the size of each hidden layer is 768, whereas in BERT-large, each hidden layer has a size of 1024.\n",
    "\n",
    "3. **`A` - Number of Attention Heads:**\n",
    "   - \"A\" stands for the number of attention heads in each Transformer block. In the multi-head attention mechanism of BERT, the model's input is split into multiple heads, allowing the model to attend to different parts of the input simultaneously. More attention heads provide a richer and more diverse representation of the input data, enabling the model to capture a variety of dependencies and nuances.\n",
    "   - BERT-base is equipped with 12 attention heads per layer, while BERT-large uses 16 attention heads per layer.\n",
    "\n",
    "These parameters are integral to the Transformer architecture that BERT is built upon, and they significantly impact both the training dynamics and the performance capabilities of the model. Variations in these parameters across different versions of BERT are primarily what make some versions larger, more powerful, or slower than others. For instance, increasing the number of layers or the size of the hidden layers generally enhances the model's performance on complex tasks but also makes it more computationally expensive and slower to train and infer. This trade-off is a key consideration when choosing or designing a BERT model for specific applications or resource-limited environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddaefff",
   "metadata": {
    "papermill": {
     "duration": 0.006691,
     "end_time": "2024-05-06T08:16:01.851205",
     "exception": false,
     "start_time": "2024-05-06T08:16:01.844514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Bert Base\n",
    "In this practice we will work with the Pre-Trained model of `BERT BASE`. It uses L=12 hidden layers with size of H=768, and A=12 attention heads. Check the model at [this page](https://www.kaggle.com/models/tensorflow/bert/tensorFlow2/en-uncased-l-12-h-768-a-12/4.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f670493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:01.868346Z",
     "iopub.status.busy": "2024-05-06T08:16:01.867646Z",
     "iopub.status.idle": "2024-05-06T08:16:17.130580Z",
     "shell.execute_reply": "2024-05-06T08:16:17.129774Z"
    },
    "papermill": {
     "duration": 15.274174,
     "end_time": "2024-05-06T08:16:17.132811",
     "exception": false,
     "start_time": "2024-05-06T08:16:01.858637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 08:16:04.100376: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-06 08:16:04.100484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-06 08:16:04.280517: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632825b",
   "metadata": {
    "papermill": {
     "duration": 0.007467,
     "end_time": "2024-05-06T08:16:17.147737",
     "exception": false,
     "start_time": "2024-05-06T08:16:17.140270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1 Specific BERT Preprocessing\n",
    "BERT has specific requirements for how text must be formatted before it can be processed by the model. This includes tokenizing the text into tokens that BERT was trained on, adding special tokens (like [CLS], [SEP]), and creating attention masks. You can access the Preprocess layer of BERT at https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a24db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:17.164699Z",
     "iopub.status.busy": "2024-05-06T08:16:17.164077Z",
     "iopub.status.idle": "2024-05-06T08:16:17.168839Z",
     "shell.execute_reply": "2024-05-06T08:16:17.167876Z"
    },
    "papermill": {
     "duration": 0.01565,
     "end_time": "2024-05-06T08:16:17.170908",
     "exception": false,
     "start_time": "2024-05-06T08:16:17.155258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_url = \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92329bf1",
   "metadata": {
    "papermill": {
     "duration": 0.008253,
     "end_time": "2024-05-06T08:16:17.187145",
     "exception": false,
     "start_time": "2024-05-06T08:16:17.178892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Crating Preprocessing Layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf4bffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:17.203877Z",
     "iopub.status.busy": "2024-05-06T08:16:17.203561Z",
     "iopub.status.idle": "2024-05-06T08:16:20.679855Z",
     "shell.execute_reply": "2024-05-06T08:16:20.679031Z"
    },
    "papermill": {
     "duration": 3.487389,
     "end_time": "2024-05-06T08:16:20.682278",
     "exception": false,
     "start_time": "2024-05-06T08:16:17.194889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching model 'tensorflow/bert/tensorflow2/en-uncased-preprocess/3' to your Kaggle notebook...\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(preprocess_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f19380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:20.700716Z",
     "iopub.status.busy": "2024-05-06T08:16:20.699974Z",
     "iopub.status.idle": "2024-05-06T08:16:21.980640Z",
     "shell.execute_reply": "2024-05-06T08:16:21.979739Z"
    },
    "papermill": {
     "duration": 1.292266,
     "end_time": "2024-05-06T08:16:21.982773",
     "exception": false,
     "start_time": "2024-05-06T08:16:20.690507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_word_ids', 'input_mask', 'input_type_ids'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['This is a spam mail', 'Computers are not unique to computers']\n",
    "text_preprocessed = bert_preprocess_model(test)\n",
    "text_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38318265",
   "metadata": {
    "papermill": {
     "duration": 0.007196,
     "end_time": "2024-05-06T08:16:21.998336",
     "exception": false,
     "start_time": "2024-05-06T08:16:21.991140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**OUTPUT Explanation**\n",
    "\n",
    "`'input_type_ids'`, `'input_word_ids'`, and `'input_mask'` these outputs are specifically formatted to meet the input requirements of BERT models. \n",
    "\n",
    "1. **`'input_word_ids'`**:\n",
    "   - These are also known as token IDs. The preprocessing model first tokenizes the text into words or subwords (subword tokenization helps in dealing with out-of-vocabulary words for which BERT hasn't been explicitly trained). Each token or subword is then mapped to a unique integer ID. These IDs are what the BERT model actually processes, and they correspond to entries in BERT's embedding table. Essentially, `'input_word_ids'` is a sequence of integers representing the tokens derived from the input text.\n",
    "\n",
    "2. **`'input_mask'`**:\n",
    "   - This is also referred to as the attention mask. The purpose of the `'input_mask'` is to provide the model with information about which parts of the input data are actual tokens and which parts are padding. This distinction is important because BERT processes fixed-size input sequences. If a given sentence or input is shorter than the maximum sequence length, it's padded with zeros. The attention mask has a binary value (0 or 1):\n",
    "     - **1** indicates that the corresponding token is a real token that should be attended to.\n",
    "     - **0** indicates that it is padding and should not be considered in the attention calculations in the Transformer model.\n",
    "\n",
    "3. **`'input_type_ids'`**:\n",
    "   - These are often referred to as segment IDs. BERT can take multiple sentences as input, which is useful for tasks that involve understanding the relationship between sentences (like question answering or natural language inference). The `'input_type_ids'` signal to the model which part of the input belongs to sentence A and which part belongs to sentence B. Typically:\n",
    "     - **0** might be used for tokens belonging to the first sentence.\n",
    "     - **1** might be used for tokens belonging to the second sentence.\n",
    "   - If there's only one input sentence, the segment IDs might be all zeros. This segmentation helps BERT understand sentence boundaries within the input.\n",
    "\n",
    "These three components (`'input_word_ids'`, `'input_mask'`, and `'input_type_ids'`) together form the complete input representation for the BERT model, allowing it to properly process and understand the text data provided to it, regardless of the specific task or data characteristics. Each plays a crucial role in ensuring that the model's attention mechanisms focus correctly on the meaningful parts of the input data while handling different input lengths and multi-sentence inputs effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c117199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:22.014936Z",
     "iopub.status.busy": "2024-05-06T08:16:22.014624Z",
     "iopub.status.idle": "2024-05-06T08:16:22.024612Z",
     "shell.execute_reply": "2024-05-06T08:16:22.023769Z"
    },
    "papermill": {
     "duration": 0.020308,
     "end_time": "2024-05-06T08:16:22.026537",
     "exception": false,
     "start_time": "2024-05-06T08:16:22.006229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
       " array([[  101,  2023,  2003,  1037, 12403,  2213,  5653,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  7588,  2024,  2025,  4310,  2000,  7588,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], dtype=int32)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed['input_mask'], text_preprocessed['input_type_ids'], text_preprocessed['input_word_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf13a5",
   "metadata": {
    "papermill": {
     "duration": 0.007692,
     "end_time": "2024-05-06T08:16:22.042103",
     "exception": false,
     "start_time": "2024-05-06T08:16:22.034411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Pay attention to `101` known as CLS token and `102` which is known as SEP token. BERT uses these as special tokens at the begining and ending of each setence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff0150",
   "metadata": {
    "papermill": {
     "duration": 0.007482,
     "end_time": "2024-05-06T08:16:22.057299",
     "exception": false,
     "start_time": "2024-05-06T08:16:22.049817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2 BERT BASE Pre-Trained Model\n",
    "You can find BERT Pre-Trained model at https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/4\n",
    "\n",
    "Let's create the `BERT BASE` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd0e4f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:22.074034Z",
     "iopub.status.busy": "2024-05-06T08:16:22.073748Z",
     "iopub.status.idle": "2024-05-06T08:16:22.077732Z",
     "shell.execute_reply": "2024-05-06T08:16:22.076876Z"
    },
    "papermill": {
     "duration": 0.014785,
     "end_time": "2024-05-06T08:16:22.079720",
     "exception": false,
     "start_time": "2024-05-06T08:16:22.064935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_url = \"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-l-12-h-768-a-12/4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbcd158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:22.096703Z",
     "iopub.status.busy": "2024-05-06T08:16:22.095842Z",
     "iopub.status.idle": "2024-05-06T08:16:37.923601Z",
     "shell.execute_reply": "2024-05-06T08:16:37.922759Z"
    },
    "papermill": {
     "duration": 15.838795,
     "end_time": "2024-05-06T08:16:37.925993",
     "exception": false,
     "start_time": "2024-05-06T08:16:22.087198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching model 'tensorflow/bert/tensorflow2/en-uncased-l-12-h-768-a-12/4' to your Kaggle notebook...\n"
     ]
    }
   ],
   "source": [
    "bert_model = hub.KerasLayer(encoder_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce35c17",
   "metadata": {
    "papermill": {
     "duration": 0.009602,
     "end_time": "2024-05-06T08:16:37.944065",
     "exception": false,
     "start_time": "2024-05-06T08:16:37.934463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, we feed the preprocessed input to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9499fe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:37.961075Z",
     "iopub.status.busy": "2024-05-06T08:16:37.960754Z",
     "iopub.status.idle": "2024-05-06T08:16:39.804100Z",
     "shell.execute_reply": "2024-05-06T08:16:39.803088Z"
    },
    "papermill": {
     "duration": 1.85496,
     "end_time": "2024-05-06T08:16:39.806784",
     "exception": false,
     "start_time": "2024-05-06T08:16:37.951824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_results = bert_model(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f908d9d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:39.825582Z",
     "iopub.status.busy": "2024-05-06T08:16:39.825241Z",
     "iopub.status.idle": "2024-05-06T08:16:39.831692Z",
     "shell.execute_reply": "2024-05-06T08:16:39.830679Z"
    },
    "papermill": {
     "duration": 0.017918,
     "end_time": "2024-05-06T08:16:39.833755",
     "exception": false,
     "start_time": "2024-05-06T08:16:39.815837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder_outputs', 'pooled_output', 'default', 'sequence_output'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04fc6ff",
   "metadata": {
    "papermill": {
     "duration": 0.008419,
     "end_time": "2024-05-06T08:16:39.850620",
     "exception": false,
     "start_time": "2024-05-06T08:16:39.842201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**OUTPUT Explanation**\n",
    "\n",
    "`['encoder_outputs', 'sequence_output', 'pooled_output', 'default']` these are part of the output from a BERT model after it processes input text. Each of these keys provides a different type of output from the BERT layers, useful for various downstream tasks in NLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc803a5",
   "metadata": {
    "papermill": {
     "duration": 0.008592,
     "end_time": "2024-05-06T08:16:39.868105",
     "exception": false,
     "start_time": "2024-05-06T08:16:39.859513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. **`'encoder_outputs'`**:\n",
    "   - This key provides the outputs of each individual encoder (Transformer block) within the BERT model. The output under this key is typically a list of tensors, with each tensor representing the output from one of the Transformer blocks. This is useful for tasks that might benefit from accessing intermediate layers of the model, rather than just the final output, as different layers capture different levels of abstraction.\n",
    "   \n",
    "   - In the context of BERT, particularly the BERT-base model, the `encoder_outputs` contains 12 items which corresponds to the number of transformer blocks or layers of the model. BERT-base is designed with 12 layers, each contributing to the hierarchical understanding of the input text at various levels of abstraction.\n",
    "\n",
    "    ### Understanding Encoder Outputs:\n",
    "\n",
    "    - **Each Layer's Contribution**: In BERT and other Transformer-based models, each transformer layer (or block) processes the input text independently and contributes progressively to the final understanding of the text. Each layer captures different aspects of the text data. For instance, lower layers might focus more on syntactic representations while higher layers might capture more semantic aspects.\n",
    "\n",
    "    - **Utility of Multiple Outputs**: Accessing the output of each individual layer can be highly beneficial for certain NLP tasks. Researchers may want to analyze or use the representations from intermediate layers because these representations might be more suitable for specific tasks. For example, earlier layers might be better for tasks focused on the syntactic nuances of the text, while later layers might be more effective for tasks involving complex semantic understanding.\n",
    "\n",
    "    - **12 Outputs for 12 Layers**: Since BERT-base has 12 layers, `encoder_outputs` includes 12 separate tensors, each representing the output from one of the 12 transformer blocks. Each tensor in `encoder_outputs` is a full set of hidden states for the input sequence, as processed by a specific layer.\n",
    "\n",
    "    ### Practical Use Cases:\n",
    "\n",
    "    - **Feature Extraction**: You can extract these individual layer outputs for custom feature engineering. For example, in some specialized tasks, features from specific layers could be more informative than just using the final layer's output.\n",
    "\n",
    "    - **Custom Models**: In advanced use cases, outputs from specific layers might be combined or manipulated differently to construct custom models tailored to particular NLP tasks, such as differentiating between types of semantic meanings or enhancing model interpretability.\n",
    "\n",
    "    - **Research and Analysis**: For research purposes, analyzing how different layers of the model react to various inputs can provide insights into how BERT processes language and which layers are most critical for certain types of language understanding.\n",
    "\n",
    "    The presence of 12 `encoder_outputs` in BERT-base is thus not just a design choice but a feature that adds versatility and depth to how the model can be utilized across a wide range of natural language processing applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc797a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:39.887364Z",
     "iopub.status.busy": "2024-05-06T08:16:39.886477Z",
     "iopub.status.idle": "2024-05-06T08:16:39.892793Z",
     "shell.execute_reply": "2024-05-06T08:16:39.891811Z"
    },
    "papermill": {
     "duration": 0.018155,
     "end_time": "2024-05-06T08:16:39.894929",
     "exception": false,
     "start_time": "2024-05-06T08:16:39.876774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_results['encoder_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb4c4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:39.913924Z",
     "iopub.status.busy": "2024-05-06T08:16:39.913625Z",
     "iopub.status.idle": "2024-05-06T08:16:39.925158Z",
     "shell.execute_reply": "2024-05-06T08:16:39.924219Z"
    },
    "papermill": {
     "duration": 0.023406,
     "end_time": "2024-05-06T08:16:39.927388",
     "exception": false,
     "start_time": "2024-05-06T08:16:39.903982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=bool, numpy=\n",
       "array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results['encoder_outputs'][-1] == bert_results['sequence_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e575cc7",
   "metadata": {
    "papermill": {
     "duration": 0.009173,
     "end_time": "2024-05-06T08:16:39.945621",
     "exception": false,
     "start_time": "2024-05-06T08:16:39.936448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. **`'sequence_output'`**:\n",
    "   - The `'sequence_output'` is the output from the last layer of the BERT model for each token in the input sequence. It provides a high-dimensional representation of each token in the context of the entire input sequence. This output is often used for `token-level tasks` such as `named entity recognition (NER)` or `token-level classification`, where a prediction is required for each input token.\n",
    "    \n",
    "   - The shape of the `sequence_output` tensor as `(2, 128, 768)` from a BERT model output indicates specific dimensions related to the model's processing of input text. \n",
    "\n",
    "    1. **Batch Size (2)**:\n",
    "       - The first dimension, `2`, indicates that the output is for a batch of two input sequences. When processing multiple sequences at once, models like BERT can handle them in batches, which is more efficient than processing each sequence individually. This allows for parallel computation and faster processing times. The number `2` here simply means there are two sequences in this batch.\n",
    "\n",
    "    2. **Sequence Length (128)**:\n",
    "       - The second dimension, `128`, represents the sequence length, that is, the number of tokens (words or subwords) in each input sequence that the model processes. Each sequence is padded or truncated to this fixed length to ensure uniformity in input size, which is required for batch processing in neural networks. The length of `128` tokens is a common choice because it balances computational efficiency with sufficient context for most NLP tasks.\n",
    "\n",
    "    3. **Hidden Size (768)**:\n",
    "       - The third dimension, `768`, is the size of the hidden layers in the BERT model. This number indicates the dimensionality of the output vectors that BERT generates for each token in the input sequence. Each token's output vector is a 768-dimensional representation that encapsulates the contextual relationships learned by the model during training. This size corresponds to the `H` parameter in the BERT-base model (as opposed to BERT-large, which would have 1024).\n",
    "\n",
    "   - **Contextual Token Representations**: Each of the 128 tokens in the two sequences gets a 768-dimensional vector that represents that token in context. These vectors are what downstream tasks (e.g., token classification or feature extraction) would use.\n",
    "\n",
    "   - **Handling of Padded Tokens**: Since sequences are padded to a maximum length of 128, parts of the `sequence_output` may correspond to padding tokens, especially if the original input sequence was shorter than 128 tokens. These padding areas typically don't carry meaningful information and are often masked out during subsequent processing steps in an NLP pipeline.\n",
    "\n",
    "    This structured output allows BERT to be flexibly applied to various NLP tasks by providing a rich, contextualized embedding for each token in the input sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56dea5cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:39.966141Z",
     "iopub.status.busy": "2024-05-06T08:16:39.965824Z",
     "iopub.status.idle": "2024-05-06T08:16:39.973315Z",
     "shell.execute_reply": "2024-05-06T08:16:39.972461Z"
    },
    "papermill": {
     "duration": 0.019395,
     "end_time": "2024-05-06T08:16:39.975319",
     "exception": false,
     "start_time": "2024-05-06T08:16:39.955924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 128, 768), dtype=float32, numpy=\n",
       "array([[[ 0.02952154,  0.08941655, -0.29414886, ..., -0.28991556,\n",
       "          0.33581254,  0.8754337 ],\n",
       "        [-0.2737661 , -0.42498857, -0.37990084, ..., -0.57002264,\n",
       "          1.0774478 ,  0.5713815 ],\n",
       "        [-0.33146417, -0.35461444, -0.253891  , ..., -0.42635572,\n",
       "          0.382568  ,  0.91525745],\n",
       "        ...,\n",
       "        [ 0.16875651, -0.27586892,  0.12268022, ...,  0.01988028,\n",
       "          0.06036456,  0.4067654 ],\n",
       "        [ 0.09589738, -0.25696522,  0.05153529, ...,  0.03342544,\n",
       "          0.07288365,  0.40554154],\n",
       "        [ 0.08295452, -0.22644407,  0.1854625 , ...,  0.14761981,\n",
       "          0.06656485,  0.36203957]],\n",
       "\n",
       "       [[ 0.07296047,  0.51637626, -0.23151016, ..., -0.4113447 ,\n",
       "         -0.0308435 ,  0.84255636],\n",
       "        [-0.3008911 ,  0.5466946 , -0.2294488 , ..., -0.2608072 ,\n",
       "          0.23354402,  0.5297949 ],\n",
       "        [ 0.65989935,  0.8210852 ,  0.0728514 , ..., -0.40727797,\n",
       "         -0.13747925,  0.5823894 ],\n",
       "        ...,\n",
       "        [ 0.06599085,  0.4200283 ,  0.23876393, ..., -0.28586888,\n",
       "         -0.15313414,  0.565475  ],\n",
       "        [ 0.05356503,  0.36034524,  0.20824277, ..., -0.32250953,\n",
       "         -0.17014687,  0.59163177],\n",
       "        [-0.05409022,  0.32501113,  0.2541487 , ..., -0.24278012,\n",
       "         -0.18027043,  0.63793445]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results['sequence_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b4878",
   "metadata": {
    "papermill": {
     "duration": 0.008987,
     "end_time": "2024-05-06T08:16:39.993553",
     "exception": false,
     "start_time": "2024-05-06T08:16:39.984566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3. **`'pooled_output'`**:\n",
    "   - The `'pooled_output'` represents a fixed-length output vector for the entire input sequence and is usually derived from the hidden state of the first token of the sequence (which is the special `[CLS]` token in BERT). This token's final hidden state is typically used as the `\"aggregate representation\"` for classification tasks. It's processed through an additional dense layer with a `Tanh activation` function to generate the pooled output. This output is useful for classification tasks where the entire input sequence needs to be represented as a single fixed-size vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4650f6b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:16:40.013035Z",
     "iopub.status.busy": "2024-05-06T08:16:40.012760Z",
     "iopub.status.idle": "2024-05-06T08:16:40.019414Z",
     "shell.execute_reply": "2024-05-06T08:16:40.018374Z"
    },
    "papermill": {
     "duration": 0.018303,
     "end_time": "2024-05-06T08:16:40.021384",
     "exception": false,
     "start_time": "2024-05-06T08:16:40.003081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.87018895, -0.42847177, -0.38309997, ..., -0.34936565,\n",
       "        -0.6542408 ,  0.8658799 ],\n",
       "       [-0.8610027 , -0.33217645, -0.44266653, ..., -0.36746684,\n",
       "        -0.6890626 ,  0.8170812 ]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results['pooled_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3efd31",
   "metadata": {
    "papermill": {
     "duration": 0.007978,
     "end_time": "2024-05-06T08:16:40.038351",
     "exception": false,
     "start_time": "2024-05-06T08:16:40.030373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4. **`'default'`**:\n",
    "   - This output typically points to one of the other outputs as the default one that should be used for most tasks. In many implementations of BERT on TensorFlow Hub, the `'default'` key points to the `'pooled_output'` as it is the most commonly used output for various classification tasks. However, depending on the specific implementation or model variant, it might point to a different output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea3bb6",
   "metadata": {
    "papermill": {
     "duration": 0.008132,
     "end_time": "2024-05-06T08:16:40.054711",
     "exception": false,
     "start_time": "2024-05-06T08:16:40.046579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "These outputs provide flexibility in how the information processed by the BERT model can be utilized, catering to both whole-sequence tasks (like classification) and token-level tasks (like tagging or question answering). The availability of different layers' outputs (via `'encoder_outputs'`) also facilitates more advanced analyses and custom model architectures that leverage deeper or intermediate representations of the input data."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 4932971,
     "modelInstanceId": 2180,
     "sourceId": 2938,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 4910135,
     "modelInstanceId": 1882,
     "sourceId": 2580,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 44.318612,
   "end_time": "2024-05-06T08:16:43.197372",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-06T08:15:58.878760",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
