{"metadata":{"colab":{"name":"NNfrom_scratch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Network from Scratch","metadata":{}},{"cell_type":"markdown","source":"In this file we develop a Neural Network from scratch, just using mathematical libraries of numpy for implemenatation.","metadata":{"id":"oVEC5DybVbYA"}},{"cell_type":"code","source":"import math\nimport numpy as np\n\nnp.set_printoptions(precision=2, suppress=True)","metadata":{"id":"sWRbCCxaGxtN","execution":{"iopub.status.busy":"2024-03-19T11:01:12.839329Z","iopub.execute_input":"2024-03-19T11:01:12.839775Z","iopub.status.idle":"2024-03-19T11:01:12.871902Z","shell.execute_reply.started":"2024-03-19T11:01:12.839739Z","shell.execute_reply":"2024-03-19T11:01:12.870758Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Let us define a couple of activation functions (sigmoid and relu) and their derivatives.","metadata":{"id":"K7sxnOVPHAgw"}},{"cell_type":"code","source":"##############################################\n# activation functions\n##############################################\n\ndef sigmoid(x): return 1 / (1 + math.exp(-x))\n\ndef sigderiv(x): return (sigmoid(x)*(1-sigmoid(x)))\n\ndef relu(x):\n  if x >= 0: return x\n  else: return 0\n\ndef reluderiv(x):\n  if x >= 0: return 1\n  else: return 0\n\ndef activate(x): return sigmoid(x)  #relu(x)\ndef actderiv(x): return sigderiv(x) #reluderiv(x)","metadata":{"id":"qT4CbsndH_kX","execution":{"iopub.status.busy":"2024-03-19T11:01:21.994076Z","iopub.execute_input":"2024-03-19T11:01:21.994474Z","iopub.status.idle":"2024-03-19T11:01:22.002219Z","shell.execute_reply.started":"2024-03-19T11:01:21.994442Z","shell.execute_reply":"2024-03-19T11:01:22.001061Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"A neural network is just a collection of numerical vectors describing the weigths of the links at each layer. For instance, a dense layer between n input neurons and m output neurons is defined by a matrix w of dimension nxm for the weights and a vector b of dimension m for the biases. \n\nSupposing the network is dense, its architecture is fullly specified by the number of neurons at each layer. For our example, we define a shallow network with 8 input neurons,\n3 hidden neurons, and 8 output neurons, hence with dimension [8,3,8].\n\nWe initialize weights and biases with random values.","metadata":{"id":"Saa8htemIY7T"}},{"cell_type":"code","source":"##############################################\n# net parameters - Initializating the parameters\n##############################################\n\ndim = [8,3,8]\nl = len(dim)\n\nw,b = [],[]\n\nfor i in range(1, l):\n  w.append(np.random.rand(dim[i-1], dim[i]))\n  b.append(np.random.rand(dim[i]))","metadata":{"id":"oqjjvojNME14","execution":{"iopub.status.busy":"2024-03-19T11:04:21.722519Z","iopub.execute_input":"2024-03-19T11:04:21.722919Z","iopub.status.idle":"2024-03-19T11:04:21.729445Z","shell.execute_reply.started":"2024-03-19T11:04:21.722891Z","shell.execute_reply":"2024-03-19T11:04:21.728457Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Network \nFor the **backpropagation algorithm** we also need to compute, at each layer, **the weighted sum z** (inputs to activation), **the activation a**, and **the partial derivative d** of the error relative to z.\n\nWe define a version of the backpropagation algorithm working \"**on line**\", processing a single training sample (x,y) at a time, and updating the nework parameters at each iteration. The backpropagation function also return the current error  relative to (x,y).\n\nAn epoch, is a full pass of the error update on all training data; it returns the cumulative error on all data.\n\n\n**Below code Explanation:**\n\nThis code is a basic feedforward neural network with gradient descent training using stochastic gradient descent (SGD) on-line learning. \n- **Initialization**: The code initializes lists `z`, `a`, and `d`, where `z` stores the activations before applying the activation function, `a` stores the activated values after applying the activation function, and `d` stores the errors at each layer.\n\n- **Feedforward Pass**: The `update` function implements the feedforward pass of the neural network. It computes the activations of each layer using the dot product of weights and activations from the previous layer, then applies an activation function to get the activated values.\n\n- **Backpropagation**: After computing the output error (`d[l-2]`) using the desired output (`y`) and the actual output (`a[l-1]`), the code performs backpropagation to compute the errors at each hidden layer (`d[i]`). It updates these errors by propagating them backward through the network.\n\n- **Weight Update**: Finally, the code updates the weights (`w`) and biases (`b`) using the computed errors and activations from each layer.\n\n- **Training**: The `epoch` function seems to implement a single epoch of training. It iterates through the provided data, calling the `update` function for each input-output pair, accumulating the total error.\n\nThis code performs stochastic gradient descent (SGD) by updating the weights after each individual training example `(x, y)` is presented. This approach is called \"on-line\" learning because it updates the model parameters for each data point.\n\nThis code implements a basic neural network training loop with a single hidden layer and a configurable activation function.","metadata":{"id":"XFhkL6jRM4q8"}},{"cell_type":"code","source":"##############################################\n# training - on line, one input data at a time\n##############################################\n\nmu = 1\n\nz,a,d=[],[],[]\n\n## Parameter initialization\nfor i in range(0,l): \n    a.append(np.zeros(dim[i]))\n\nfor i in range(1,l):\n    z.append(np.zeros(dim[i]))\n    d.append(np.zeros(dim[i]))\n\ndef update(x,y):\n    #input                \n    a[0] = x\n    \n    #feed forward\n    for i in range(0,l-1):\n        z[i] = np.dot(a[i],w[i])+b[i]\n        a[i+1] = np.vectorize(activate)(z[i])\n  \n    #output error\n    d[l-2] = (y - a[l-1])*np.vectorize(actderiv)(z[l-2])\n  \n    #back propagation\n    for i in range(l-3,-1,-1):\n        d[i]=np.dot(w[i+1],d[i+1])*np.vectorize(actderiv)(z[i])\n  \n    #updating\n    for i in range(0,l-1):\n        for k in range (0,dim[i+1]):\n            for j in range (0,dim[i]):\n                w[i][j,k] = w[i][j,k] + mu*a[i][j]*d[i][k]\n            b[i][k] = b[i][k] + mu*d[i][k]\n        \n        if False:\n          print(\"d[%i] = %s\" % (i,(d[i],)))\n          print(\"b[%i] = %s\" % (i,(b[i],)))\n      #print(\"error = {}\".format(np.sum((y-a[l-1])**2)))  \n    return np.sum((y-a[l-1])**2)\n\ndef epoch(data):\n    e = 0\n    for (x,y) in data:\n        e += update(x,y)\n    return e","metadata":{"id":"468aM7K-OJs1","execution":{"iopub.status.busy":"2024-03-19T11:24:29.684626Z","iopub.execute_input":"2024-03-19T11:24:29.685097Z","iopub.status.idle":"2024-03-19T11:24:29.700732Z","shell.execute_reply.started":"2024-03-19T11:24:29.685063Z","shell.execute_reply":"2024-03-19T11:24:29.699549Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Training data on this Model\nNow we define same data and fit the network over them. \n\n- We want to define a simple example of autoencoder, taking in input a one-hot representation of the numbers between 0 and 7, and trying to compress them to a\n- Boolean internal representation on 3 bits.\n\n**The following code:**\n1. Defines a matrix `X` which represents a set of one-hot encoded vectors. Each row of `X` represents a different class, with each column representing a binary feature indicating the presence or absence of that feature for the corresponding class.\n\n2. Defines a function `data()` that returns a generator of paired data points, where each input is paired with itself. This function essentially creates a dataset where each input is paired with its own label.\n\n3. Defines a stopping criterion `final_error` and a loop that repeatedly calls the `epoch` function with the dataset generated by `data()` until the error (`dist`) falls below `final_error`.\n    - We have defined the epoch function previously\n    \n4. After training, it runs a forward pass for each input vector in `X`, printing out the activations of the hidden layer.\n","metadata":{"id":"oO242-WAQCY1"}},{"cell_type":"code","source":"X = [[1,0,0,0,0,0,0,0],\n     [0,1,0,0,0,0,0,0],\n     [0,0,1,0,0,0,0,0],\n     [0,0,0,1,0,0,0,0],\n     [0,0,0,0,1,0,0,0],\n     [0,0,0,0,0,1,0,0],\n     [0,0,0,0,0,0,1,0],\n     [0,0,0,0,0,0,0,1]]\n\ndef data(): \n    return zip(X,X)  \n\nfinal_error = .002\ndist = epoch(data()) \n\nwhile dist > final_error:\n#     print(\"distance= %f\" % dist)\n    dist = epoch(data())\n\nprint(\"distance= %f\" % dist)\n\nfor x in X:\n    print(\"Input = %s\" % (x,))\n    a[0] = x\n\n    #feed forward\n    for i in range(0,l-2):\n        z[i] = np.dot(a[i],w[i])+b[i]\n        a[i+1] = np.vectorize(activate)(z[i])\n  \n    print(\"Hidden level = %s\" % (a[i+1],),  \"\\n\")\n    \n    z[l-2] = np.dot(a[l-2],w[l-2])+b[l-2]\n    a[l-1] = np.vectorize(activate)(z[l-2])\n    #print(\"output = %s\" % (a[l-1],))","metadata":{"id":"i2M52bfsFXpp","cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9478cd2-e802-42d9-c6d4-83fbff86ebfc","execution":{"iopub.status.busy":"2024-03-19T11:27:55.949195Z","iopub.execute_input":"2024-03-19T11:27:55.949544Z","iopub.status.idle":"2024-03-19T11:27:55.968493Z","shell.execute_reply.started":"2024-03-19T11:27:55.949518Z","shell.execute_reply":"2024-03-19T11:27:55.967283Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"distance= 0.002000\nInput = [1, 0, 0, 0, 0, 0, 0, 0]\nHidden level = [0.01 0.02 0.01] \n\nInput = [0, 1, 0, 0, 0, 0, 0, 0]\nHidden level = [0.99 1.   1.  ] \n\nInput = [0, 0, 1, 0, 0, 0, 0, 0]\nHidden level = [0.94 0.01 0.01] \n\nInput = [0, 0, 0, 1, 0, 0, 0, 0]\nHidden level = [0.99 0.   0.97] \n\nInput = [0, 0, 0, 0, 1, 0, 0, 0]\nHidden level = [0.02 0.   0.91] \n\nInput = [0, 0, 0, 0, 0, 1, 0, 0]\nHidden level = [0.01 0.97 0.01] \n\nInput = [0, 0, 0, 0, 0, 0, 1, 0]\nHidden level = [0.98 0.98 0.  ] \n\nInput = [0, 0, 0, 0, 0, 0, 0, 1]\nHidden level = [0.   0.96 0.99] \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Exercises\n\n1.   change the specification of the network to allow a different activation function for each layer;\n2.   modify the backpropagation algorithm to work on a minibatch of samples.\n\n\n\n","metadata":{"id":"S0LkDAsuo6rD"}}]}