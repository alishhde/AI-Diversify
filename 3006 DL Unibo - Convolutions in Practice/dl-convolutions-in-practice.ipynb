{"metadata":{"colab":{"name":"mnist_conv.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Convolutions in Practice\nThis notebook is meant to introduce convolutional layers, with special emphasis on the relation between the dimension of the input tensor, the kernel size, the stride, the number of filters and the dimension of the output tensor.","metadata":{"id":"QX15PRrJq_zX"}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"id":"jjDI4eEQZ9Xc","execution":{"iopub.status.busy":"2024-03-24T17:05:37.760988Z","iopub.execute_input":"2024-03-24T17:05:37.762107Z","iopub.status.idle":"2024-03-24T17:05:53.998618Z","shell.execute_reply.started":"2024-03-24T17:05:37.762072Z","shell.execute_reply":"2024-03-24T17:05:53.997397Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-24 17:05:40.207384: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-24 17:05:40.207616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-24 17:05:40.390148: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, ZeroPadding2D, Dense, Flatten, Layer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras.datasets import mnist","metadata":{"id":"atmltv8-UZW9","execution":{"iopub.status.busy":"2024-03-24T17:05:54.000645Z","iopub.execute_input":"2024-03-24T17:05:54.001444Z","iopub.status.idle":"2024-03-24T17:05:54.016562Z","shell.execute_reply.started":"2024-03-24T17:05:54.001404Z","shell.execute_reply":"2024-03-24T17:05:54.015261Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"We run the example over the mnist data set. Keras provides a very friendly access to several renowed databases, comprising mnist, cifar10, cifar100, IMDB and many others. See https://keras.io/api/datasets/ for documentation","metadata":{"id":"MDH8iUaxrvZB"}},{"cell_type":"code","source":"import numpy as np\n(x_train, y_train), (x_test, y_test) = mnist.load_data()","metadata":{"id":"5j-DYkTaz3Ts","colab":{"base_uri":"https://localhost:8080/"},"outputId":"702f538c-fba9-4bc2-f358-49815630ff7c","execution":{"iopub.status.busy":"2024-03-24T17:05:54.018131Z","iopub.execute_input":"2024-03-24T17:05:54.018507Z","iopub.status.idle":"2024-03-24T17:05:55.410197Z","shell.execute_reply.started":"2024-03-24T17:05:54.018478Z","shell.execute_reply":"2024-03-24T17:05:55.408988Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Mnist images are grayscale images with pixels in the range [0,255].\nWe pass to floats, and normalize them in the range [0,1].","metadata":{"id":"nOMU1JxB0BRH"}},{"cell_type":"code","source":"x_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.","metadata":{"id":"G78aNHyG0bWD","execution":{"iopub.status.busy":"2024-03-24T17:05:55.412411Z","iopub.execute_input":"2024-03-24T17:05:55.412844Z","iopub.status.idle":"2024-03-24T17:05:55.581639Z","shell.execute_reply.started":"2024-03-24T17:05:55.412812Z","shell.execute_reply":"2024-03-24T17:05:55.580533Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Bidimensional convolutions expect input with three dimensions (plus an additional batchsize dimension): width, height, channels. \nSince mnist digits have only two dimensions (being in grayscale), we need to extend them with an additional dimension.","metadata":{"id":"5aYDPNGn0n73"}},{"cell_type":"code","source":"(n,w,h) = x_train.shape\nx_train = x_train.reshape(n,w,h,1)\n(n,w,h) = x_test.shape\nx_test = x_test.reshape(n,w,h,1)\nprint(x_train.shape)\nprint(x_test.shape)","metadata":{"id":"koAbxpngVCsq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a4fb67c-ce8b-44ff-964d-e0bf4d0c8b0a","execution":{"iopub.status.busy":"2024-03-24T17:05:55.583711Z","iopub.execute_input":"2024-03-24T17:05:55.584176Z","iopub.status.idle":"2024-03-24T17:05:55.591137Z","shell.execute_reply.started":"2024-03-24T17:05:55.584139Z","shell.execute_reply":"2024-03-24T17:05:55.589805Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(60000, 28, 28, 1)\n(10000, 28, 28, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Mnist labels are integers in the range [0,9]. Since the network will produce probabilities for each one of these categories, if we want to compare it with the ground trouth probability using categorical crossentropy, that is the traditional choice, we should change each integer in its categorical description, using e.g. the \"`to_categorical`\" function in utils.\n\nAlternatively, we can use the so called [\"sparse categorical crossentropy\"](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) loss function that allows us to directly compare predictions with labels.\n\nCategorical crossentropy and sparse categorical crossentropy are both loss functions commonly used in classification tasks in deep learning. They are similar in their purpose of measuring the difference between the predicted probability distribution and the true distribution of class labels, but they differ in how they handle the representation of the target labels.\n\n### Categorical Crossentropy:\n\n1. **Input**: The output of the neural network is typically a probability distribution over the classes, generated by the softmax activation function. Each value in the output vector represents the probability of the input belonging to the corresponding class.\n\n2. **Target**: The target labels are provided in one-hot encoded format, where each target label is represented as a binary vector with a 1 in the position corresponding to the true class and 0s elsewhere.\n\n3. **Loss Calculation**: Categorical crossentropy calculates the cross-entropy loss between the predicted probability distribution and the true one-hot encoded target labels.\n\n4. **Loss Function**: The formula for categorical crossentropy loss for a single sample is:\n\n   $$\\text{Loss} = -\\sum_{i} y_{\\text{true}}[i] \\cdot \\log(\\text{softmax}(y_{\\text{pred}})[i])$$\n\n   Where:\n   - $y_{\\text{true}}$ is the true one-hot encoded target label.\n   - $\\text{softmax}(y_{\\text{pred}})$ is the predicted probability distribution over classes.\n\n5. **Overall Loss**: The loss is averaged over all samples in the batch to obtain the overall loss for that batch.\n\n### Sparse Categorical Crossentropy:\n\nSparse categorical crossentropy is particularly useful when the target labels are provided as integers rather than one-hot encoded vectors. It eliminates the need for explicit conversion of target labels to one-hot encoded format.\n\n1. **Input**: Same as categorical crossentropy.\n\n2. **Target**: The target labels are provided as integers, where each integer represents the index of the true class for each sample.\n\n3. **Loss Calculation**: Sparse categorical crossentropy computes the cross-entropy loss between the predicted probability distribution and a one-hot encoded version of the true class labels (implicitly converting the integer labels to one-hot encoded format during computation).\n\n4. **Loss Function**: The formula for sparse categorical crossentropy loss for a single sample is similar to categorical crossentropy but handles integer labels directly:\n\n   $$\\text{Loss} = -\\log(\\text{softmax}(y_{\\text{true}})[\\text{true_class}])$$\n\n   Where:\n   - $\\text{softmax}(y_{\\text{true}})$ is the predicted probability distribution over classes.\n   - $\\text{true_class}$ is the index of the true class.\n\n5. **Overall Loss**: The loss is averaged over all samples in the batch.\n\n### Comparison:\n\n- **Representation of Target Labels**: Categorical crossentropy requires target labels in one-hot encoded format, while sparse categorical crossentropy accepts integer labels directly.\n- **Memory Efficiency**: Sparse categorical crossentropy is more memory-efficient, especially when dealing with a large number of classes, as it avoids the need to one-hot encode target labels.\n- **Computation Efficiency**: Sparse categorical crossentropy might be slightly more computationally efficient because it avoids the explicit conversion of target labels to one-hot encoded format.\n- **Usage**: Choose categorical crossentropy when target labels are in one-hot encoded format, and sparse categorical crossentropy when target labels are integers.\n","metadata":{"id":"mOp2WtJJsrpn"}},{"cell_type":"code","source":"# y_train = keras.utils.to_categorical(y_train)\n# y_test = keras.utils.to_categorical(y_test)","metadata":{"id":"ZK--l9nzs9F-","execution":{"iopub.status.busy":"2024-03-24T17:06:11.165985Z","iopub.execute_input":"2024-03-24T17:06:11.166861Z","iopub.status.idle":"2024-03-24T17:06:11.172083Z","shell.execute_reply.started":"2024-03-24T17:06:11.166825Z","shell.execute_reply":"2024-03-24T17:06:11.170824Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Let us come to the convolutional network. We define a simple network composed by three convolutional layers, followed by a couple of Dense layers.","metadata":{"id":"ZO6JwhwnurFi"}},{"cell_type":"code","source":"xin = Input(shape=(28,28,1))\nx = Conv2D(16,(3,3),strides=(2,2),padding='valid')(xin)\nx = Conv2D(32,(3,3),strides=(2,2),padding='valid')(x)\nx = Conv2D(64,(3,3),strides=(2,2),padding='valid')(x)\nx = Flatten()(x)\nx = Dense(64, activation ='relu')(x)\nres = Dense(10,activation = 'softmax')(x)\n\nmynet = Model(inputs=xin,outputs=res)","metadata":{"id":"hVUe816fUyu6","execution":{"iopub.status.busy":"2024-03-24T17:22:10.912187Z","iopub.execute_input":"2024-03-24T17:22:10.913056Z","iopub.status.idle":"2024-03-24T17:22:10.969977Z","shell.execute_reply.started":"2024-03-24T17:22:10.913014Z","shell.execute_reply":"2024-03-24T17:22:10.968872Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### Code Explanation\nThis code defines a convolutional neural network (CNN).\n\n- `Input(shape=(28,28,1))`: This line creates an input layer for the network. The `shape` parameter specifies the shape of the input data. In this case, it indicates that the input data will be 4D tensors with a shape of (batch_size, 28, 28, 1), where `batch_size` is the number of samples in each batch. This suggests that the input data consists of 28x28 images with 1 channels (grayscale).\n- `Conv2D(16, (3,3), strides=(2,2), padding='valid')`: This line creates a convolutional layer with 16 filters, each with a 3x3 kernel. \n    - The `strides` parameter determines the step size of the sliding window during convolution. Here, it's set to (2,2), meaning the filter moves 2 pixels at a time in both the height and width dimensions. \n    - The `padding` parameter determines how the input is padded. `'valid'` padding means no padding is added to the input.\n    - `(xin)`: This layer is applied to the `xin` input layer defined earlier.\n- Similar to the previous line, the next two lines create additional convolutional layers with increasing numbers of filters (32 and 64, respectively). Each layer applies convolution to the output of the previous layer (`x`), resulting in a sequence of feature maps.\n- `Flatten()`: This line adds a flatten layer to the network. It reshapes the 3D output tensor from the previous convolutional layers into a 1D tensor, which is required as input for the subsequent fully connected layers.\n- `Dense(64, activation='relu')`: This line adds a fully connected (dense) layer with 64 neurons and ReLU activation function. It connects every neuron in the previous layer to every neuron in this layer.\n- `Dense(10, activation='softmax')`: This line adds another fully connected layer with 10 neurons (since there are 10 output classes) and softmax activation function. Softmax converts the output values into probabilities representing the likelihood of each class.\n- `Model(inputs=xin, outputs=res)`: This line creates a Keras `Model` by specifying the input (`xin`) and output (`res`) layers. This model represents the entire neural network architecture defined above.\n\n#### Why we don't have activation function for the Convolution layers?\nThis is not uncommon, and there are reasons why activation functions are often omitted or deferred in convolutional neural networks (CNNs):\n\n1. **Parameter Efficiency**: Convolutional layers already introduce non-linearity through their convolution operation. This non-linearity comes from the element-wise multiplication and summation of filter weights and input values. Adding an additional activation function immediately after the convolution operation may not significantly improve the model's representational power.\n\n2. **Learning Representations**: CNNs are designed to automatically learn hierarchical representations from input data. The convolution operation itself allows the network to learn complex features and patterns. Activation functions are primarily used to introduce non-linearity and help the network learn more complex functions. In CNNs, the convolution operation inherently introduces non-linearity, making explicit activation functions less crucial.\n\n3. **Gradient Flow**: Certain activation functions, such as ReLU, can suffer from the \"dying ReLU\" problem, where neurons can become inactive during training and stop learning. By omitting activation functions in convolutional layers, this problem can be mitigated, as the convolution operation itself allows gradients to flow through the network more easily.\n\n4. **Model Flexibility**: Omitting activation functions in convolutional layers allows for greater flexibility in network design. Researchers and practitioners may experiment with different activation functions or even stacking multiple layers without activations to explore different architectures and achieve better performance.\n\nHowever, it's important to note that the absence of activation functions in convolutional layers doesn't mean they're not used at all in CNNs. Activation functions are commonly used in fully connected layers and sometimes after certain convolutional layers, especially in deeper architectures where introducing additional non-linearity can be beneficial for learning complex representations. Additionally, certain architectures or tasks may benefit from using activation functions in convolutional layers. It often depends on the specific problem being addressed and the empirical performance of the model during training and evaluation.","metadata":{}},{"cell_type":"markdown","source":"Now let's have a look at the summary","metadata":{"id":"wBDN-sBi7uUO"}},{"cell_type":"code","source":"mynet.summary()","metadata":{"id":"sgWQT4jHZUR3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"54b39bb6-605b-4821-bc45-9c46df26fffc","execution":{"iopub.status.busy":"2024-03-24T17:22:11.868308Z","iopub.execute_input":"2024-03-24T17:22:11.868703Z","iopub.status.idle":"2024-03-24T17:22:11.894077Z","shell.execute_reply.started":"2024-03-24T17:22:11.868676Z","shell.execute_reply":"2024-03-24T17:22:11.892845Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m160\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,640\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m18,496\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m40,394\u001b[0m (157.79 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,394</span> (157.79 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m40,394\u001b[0m (157.79 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,394</span> (157.79 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"- As we said earlier, in valid mode, no padding is applied. \n\n    Along each axis, the output dimension O is computed from the input dimension I using the formula O=(I-K)/S +1, where K is the kernel dimension and S is the stride. \n    \n    For all layers, K=3 and S=2. So, for the first conv we pass from dimension 28 to dimension (28-3)/2+1 = 13, then to dimension (13-3)/2+1 = 6 and finally to dimension (6-3)/2+1 = 2. \n\n    - For practice you can modify \"valid\" to \"same\" and see what happens.\n\n\n- The second important point is about the number of parameters. You must keep in mind that a kernel of dimension K1 x K2 has an actual dimension K1 x K2 x CI, where CI is number of input channels: in other words the kernel is computing at the same time spatial and cross-channel correlations.\n\n    So, for the first convolution, we have 3 x 3 x 1 + 1 = 10 parameters for each filter (1 for the bias), and since we are computing 16 filters, the number of parameters is 10 x 16 = 160.\n    \n    For the second convolution, each filter has 3 x 3 x 16 + 1 = 145 parameters, ans since we have 32 filters, the total number of parameters is 145 x 32 = 4640.\n\n","metadata":{"id":"iiFJ6HMQ8icY"}},{"cell_type":"markdown","source":"Let us come to training.\n\nIn addition to the optimizer and the loss, we also pass a \"metrics\" argument. Metrics are additional functions that are not directly used for training, but allows us to monitor its advancement. For instance, we use accuracy, in this case (sparse, because we are using labels, and cateogrical because we have multiple categories).","metadata":{"id":"Lj77T0xDBbkc"}},{"cell_type":"code","source":"mynet.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=[metrics.SparseCategoricalAccuracy()])","metadata":{"id":"5woK9FZhd2CA","execution":{"iopub.status.busy":"2024-03-24T17:22:13.389329Z","iopub.execute_input":"2024-03-24T17:22:13.389785Z","iopub.status.idle":"2024-03-24T17:22:13.402378Z","shell.execute_reply.started":"2024-03-24T17:22:13.389730Z","shell.execute_reply":"2024-03-24T17:22:13.401208Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"mynet.fit(x_train, y_train, shuffle=True, epochs=10, batch_size=32, validation_data=(x_test,y_test))","metadata":{"id":"flvXXtQwbvFR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"620a2e82-bec1-4e48-a329-cd8eb4115e63","execution":{"iopub.status.busy":"2024-03-24T17:22:13.691780Z","iopub.execute_input":"2024-03-24T17:22:13.692223Z","iopub.status.idle":"2024-03-24T17:23:47.841970Z","shell.execute_reply.started":"2024-03-24T17:22:13.692192Z","shell.execute_reply":"2024-03-24T17:23:47.840539Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.4527 - sparse_categorical_accuracy: 0.8575 - val_loss: 0.1353 - val_sparse_categorical_accuracy: 0.9571\nEpoch 2/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.1360 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.1243 - val_sparse_categorical_accuracy: 0.9603\nEpoch 3/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.1080 - sparse_categorical_accuracy: 0.9676 - val_loss: 0.1063 - val_sparse_categorical_accuracy: 0.9666\nEpoch 4/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0912 - sparse_categorical_accuracy: 0.9716 - val_loss: 0.0971 - val_sparse_categorical_accuracy: 0.9707\nEpoch 5/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0817 - sparse_categorical_accuracy: 0.9741 - val_loss: 0.0895 - val_sparse_categorical_accuracy: 0.9713\nEpoch 6/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0777 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.0901 - val_sparse_categorical_accuracy: 0.9712\nEpoch 7/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0710 - sparse_categorical_accuracy: 0.9778 - val_loss: 0.0944 - val_sparse_categorical_accuracy: 0.9721\nEpoch 8/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0656 - sparse_categorical_accuracy: 0.9785 - val_loss: 0.0963 - val_sparse_categorical_accuracy: 0.9711\nEpoch 9/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.0624 - sparse_categorical_accuracy: 0.9801 - val_loss: 0.1057 - val_sparse_categorical_accuracy: 0.9688\nEpoch 10/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0549 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.0848 - val_sparse_categorical_accuracy: 0.9756\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b1628382e30>"},"metadata":{}}]}]}