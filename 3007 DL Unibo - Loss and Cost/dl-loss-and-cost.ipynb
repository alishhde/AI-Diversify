{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303ddd56",
   "metadata": {
    "papermill": {
     "duration": 0.005371,
     "end_time": "2024-03-30T15:29:40.529595",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.524224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss and Cost\n",
    "In this notebook we shall discuss the Loss and Cost of the process of training an algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7934de6",
   "metadata": {
    "papermill": {
     "duration": 0.004605,
     "end_time": "2024-03-30T15:29:40.539446",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.534841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Cost and loss are related but slightly different concepts.\n",
    "\n",
    "1. **Cost Function**:\n",
    "   - **Definition**: The cost function (also known as the objective function or the error function) quantifies the `\"cost\"` associated with the errors made by the model in its predictions.\n",
    "   - **Purpose**: It is used during the training phase to guide the optimization algorithm in adjusting the model's parameters to minimize this cost.\n",
    "   - **Methods**: Different types of cost functions can be used depending on the specific task and the nature of the data. Some common cost functions include Mean Squared Error (MSE), Cross-Entropy Loss, Hinge Loss, etc.\n",
    "   - **Example**: In linear regression, the cost function is typically the Mean Squared Error (MSE), which calculates the average squared difference between the actual and predicted values.\n",
    "\n",
    "2. **Loss Function**:\n",
    "   - **Definition**: The loss function is a component of the cost function. It measures the difference between the predicted values of the model and the actual target values for a single data point.\n",
    "   - **Purpose**: It provides feedback to the optimization algorithm during training, indicating how well the model is performing on individual data points.\n",
    "   - **Methods**: Similar to cost functions, there are various types of loss functions used in different contexts. Some common loss functions include Mean Absolute Error (MAE), Binary Cross-Entropy Loss, Categorical Cross-Entropy Loss, etc.\n",
    "   - **Example**: In binary classification tasks, the Binary Cross-Entropy Loss is often used as the loss function. It penalizes the model based on the divergence between the predicted probabilities and the actual binary labels.\n",
    "\n",
    "In summary, cost function is a broader concept that encompasses the overall measure of error or cost associated with the model's predictions across the entire dataset, while loss function evaluates the performance of the model on individual data points and is a component of the cost function. The choice of cost and loss functions depends on the specific problem being addressed and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc08504",
   "metadata": {
    "papermill": {
     "duration": 0.004519,
     "end_time": "2024-03-30T15:29:40.548920",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.544401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Although in the frameworks that are provided for the AI algorithm training these cost and loss are already implemented, in this notebook we will try to implement them ourself to get a better idea of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9737a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.561698Z",
     "iopub.status.busy": "2024-03-30T15:29:40.560891Z",
     "iopub.status.idle": "2024-03-30T15:29:40.571454Z",
     "shell.execute_reply": "2024-03-30T15:29:40.570491Z"
    },
    "papermill": {
     "duration": 0.02009,
     "end_time": "2024-03-30T15:29:40.573966",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.553876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5822e7d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.585586Z",
     "iopub.status.busy": "2024-03-30T15:29:40.585184Z",
     "iopub.status.idle": "2024-03-30T15:29:40.590521Z",
     "shell.execute_reply": "2024-03-30T15:29:40.589199Z"
    },
    "papermill": {
     "duration": 0.01391,
     "end_time": "2024-03-30T15:29:40.592763",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.578853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_predicted = np.array([1,1,0,0,1])\n",
    "y_true = np.array([0.30,0.7,1,0,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c424a",
   "metadata": {
    "papermill": {
     "duration": 0.004668,
     "end_time": "2024-03-30T15:29:40.602407",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.597739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='color:Red'>Mean Absolute Error</h3>\n",
    "Mean Absolute Error (MAE) is a common metric used in regression analysis to evaluate the performance of a predictive model. It measures the average absolute difference between the predicted values and the actual values in a dataset.\n",
    "\n",
    "__Steps:__\n",
    "\n",
    "1. For each data point in the dataset, calculate the absolute difference between the predicted value $(\\hat{y}_i)$ and the actual value $(y_i)$.\n",
    "   \n",
    "   $$\\text{Absolute Difference} = | \\hat{y}_i - y_i |$$\n",
    "\n",
    "2. Sum up all these absolute differences.\n",
    "\n",
    "   $$\\text{Sum of Absolute Differences} = \\sum_{i=1}^{n} | \\hat{y}_i - y_i |$$\n",
    "\n",
    "3. Calculate the mean of these absolute differences by dividing the sum by the total number of data points $(n)$.\n",
    "\n",
    "   $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | \\hat{y}_i - y_i |$$\n",
    "\n",
    "MAE is useful because it provides a straightforward interpretation of the average prediction error in the same units as the target variable. For example, if you're predicting house prices, a MAE of $10,000$ would mean that, on average, your predictions are off by $10,000$.\n",
    "\n",
    "__Advantages of using MAE__:\n",
    "\n",
    "1. It's easy to understand and interpret.\n",
    "2. It's less sensitive to outliers compared to other metrics like Mean Squared Error (MSE), which squares the differences.\n",
    "3. It's useful when the distribution of the target variable is skewed or has outliers.\n",
    "\n",
    "However, MAE does not penalize large errors as heavily as MSE does. Consequently, in some scenarios, such as when large errors are particularly undesirable, MSE might be a more appropriate metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af49581c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.614601Z",
     "iopub.status.busy": "2024-03-30T15:29:40.614169Z",
     "iopub.status.idle": "2024-03-30T15:29:40.619792Z",
     "shell.execute_reply": "2024-03-30T15:29:40.618525Z"
    },
    "papermill": {
     "duration": 0.01474,
     "end_time": "2024-03-30T15:29:40.622051",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.607311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mae(y_predicted, y_true):\n",
    "    total_error = 0\n",
    "    for yp, yt in zip(y_predicted, y_true):\n",
    "        # Step 1, 2\n",
    "        total_error += abs(yp - yt)\n",
    "    print(\"Total error is:\",total_error)\n",
    "    \n",
    "    # Step 3\n",
    "    mae = total_error/len(y_predicted)\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fcb2b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.634198Z",
     "iopub.status.busy": "2024-03-30T15:29:40.633052Z",
     "iopub.status.idle": "2024-03-30T15:29:40.640164Z",
     "shell.execute_reply": "2024-03-30T15:29:40.638595Z"
    },
    "papermill": {
     "duration": 0.015369,
     "end_time": "2024-03-30T15:29:40.642390",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.627021",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error is: 2.5\n",
      "Mean absolute error is: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean absolute error is:\", mae(y_predicted, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928a810",
   "metadata": {
    "papermill": {
     "duration": 0.005264,
     "end_time": "2024-03-30T15:29:40.653859",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.648595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Implement same thing using numpy in much easier way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad50d0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.666730Z",
     "iopub.status.busy": "2024-03-30T15:29:40.665893Z",
     "iopub.status.idle": "2024-03-30T15:29:40.670464Z",
     "shell.execute_reply": "2024-03-30T15:29:40.669608Z"
    },
    "papermill": {
     "duration": 0.013309,
     "end_time": "2024-03-30T15:29:40.672598",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.659289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mae_np(y_predicted, y_true):\n",
    "    return np.mean(np.abs(y_predicted-y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dacd878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.684522Z",
     "iopub.status.busy": "2024-03-30T15:29:40.684087Z",
     "iopub.status.idle": "2024-03-30T15:29:40.691950Z",
     "shell.execute_reply": "2024-03-30T15:29:40.690785Z"
    },
    "papermill": {
     "duration": 0.016274,
     "end_time": "2024-03-30T15:29:40.693895",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.677621",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_np(y_predicted, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c467ae2",
   "metadata": {
    "papermill": {
     "duration": 0.004816,
     "end_time": "2024-03-30T15:29:40.703842",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.699026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='color:red'>Log Loss or Binary Cross Entropy</h3>\n",
    "Binary Cross-Entropy Loss, also known as Log Loss, is a common loss function used in binary classification tasks. It measures the discrepancy between the predicted probabilities output by the model and the actual binary labels of the data.\n",
    "\n",
    "__Steps:__\n",
    "\n",
    "1. For each data point in the dataset, calculate the cross-entropy loss using the predicted probability $( \\hat{y}_i ) $ for the positive class and the actual binary label $( y_i )$.\n",
    "   \n",
    "   $$\\text{Cross-Entropy Loss} = -\\left( y_i \\log(\\hat{y}_i + \\epsilon) + (1 - y_i) \\log(1 - \\hat{y}_i + \\epsilon) \\right)$$\n",
    "\n",
    "   Where:\n",
    "   - $ \\hat{y}_i $ is the predicted probability for the positive class (typically obtained from a sigmoid function applied to the model's output).\n",
    "   - $ y_i $ is the actual binary label (0 for the negative class, 1 for the positive class).\n",
    "   - The epsilon term is often added to the logarithmic functions to prevent numerical instability when the predicted probabilities are very close to 0 or 1. It is typically a small positive constant, such as $10^{-7}$.\n",
    "\n",
    "2. Average the cross-entropy losses over all data points in the dataset to obtain the overall Binary Cross-Entropy Loss.\n",
    "\n",
    "   $$ \\text{Binary Cross-Entropy Loss} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( -\\left( y_i \\log(\\hat{y}_i + \\epsilon) + (1 - y_i) \\log(1 - \\hat{y}_i + \\epsilon) \\right) \\right) $$\n",
    "\n",
    "Binary Cross-Entropy Loss essentially quantifies how well the predicted probabilities match the true labels. If the predicted probability for the positive class $( \\hat{y}_i )$ is close to 1 when the actual label $( y_i )$ is 1 and close to 0 when the actual label is 0, the loss will be low. Conversely, if the predicted probability diverges significantly from the true label, the loss will be high.\n",
    "\n",
    "**Advantages of using Binary Cross-Entropy Loss:**\n",
    "\n",
    "1. It penalizes confident wrong predictions heavily, which encourages the model to output probabilities close to 1 for positive examples and close to 0 for negative examples.\n",
    "2. It provides a smooth and differentiable loss function, making it suitable for gradient-based optimization algorithms like stochastic gradient descent (SGD).\n",
    "\n",
    "Binary Cross-Entropy Loss is commonly used in binary classification tasks, where the goal is to classify instances into one of two classes. It's worth noting that there are variations of cross-entropy loss for multi-class classification tasks, such as Categorical Cross-Entropy Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3302469f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.716039Z",
     "iopub.status.busy": "2024-03-30T15:29:40.715383Z",
     "iopub.status.idle": "2024-03-30T15:29:40.720942Z",
     "shell.execute_reply": "2024-03-30T15:29:40.719976Z"
    },
    "papermill": {
     "duration": 0.014352,
     "end_time": "2024-03-30T15:29:40.723204",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.708852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_predicted):\n",
    "    epsilon = 1e-7\n",
    "    y_predicted_new = [max(i, epsilon) for i in y_predicted]\n",
    "    y_predicted_new = [min(i, 1 - epsilon) for i in y_predicted_new]\n",
    "    y_predicted_new = np.array(y_predicted_new)\n",
    "    return -np.mean(y_true * np.log(y_predicted_new) + (1 - y_true) * np.log(1 - y_predicted_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff29707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.735970Z",
     "iopub.status.busy": "2024-03-30T15:29:40.735523Z",
     "iopub.status.idle": "2024-03-30T15:29:40.742780Z",
     "shell.execute_reply": "2024-03-30T15:29:40.741536Z"
    },
    "papermill": {
     "duration": 0.016818,
     "end_time": "2024-03-30T15:29:40.745369",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.728551",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.059047875637068"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_true, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c4fc6",
   "metadata": {
    "papermill": {
     "duration": 0.004853,
     "end_time": "2024-03-30T15:29:40.755504",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.750651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style='color:red'>Mean Squared Error</h3>\n",
    "Mean Squared Error (MSE) is a common metric used in regression tasks to evaluate the performance of a predictive model. It measures the average of the squares of the differences between the predicted values and the actual values.\n",
    "\n",
    "\n",
    "__Steps:__\n",
    "\n",
    "1. For each data point in the dataset, calculate the squared difference between the predicted value $ ( \\hat{y}_i ) $ and the actual value $( y_i )$.\n",
    "   \n",
    "   $$ \\text{Squared Difference} = (\\hat{y}_i - y_i)^2 $$\n",
    "\n",
    "2. Sum up all these squared differences.\n",
    "\n",
    "   $$ \\text{Sum of Squared Differences} = \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 $$\n",
    "\n",
    "3. Calculate the mean of these squared differences by dividing the sum by the total number of data points $( n )$.\n",
    "\n",
    "   $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 $$\n",
    "\n",
    "MSE provides a measure of the average squared deviation between the predicted values and the actual values. It's commonly used because it has several desirable properties, including being continuous, differentiable, and sensitive to large errors due to the squaring operation.\n",
    "\n",
    "Advantages of using MSE:\n",
    "\n",
    "1. It's easy to understand and interpret. The units of MSE are the square of the units of the target variable.\n",
    "2. It penalizes larger errors more heavily than smaller errors, which can be desirable in many regression tasks.\n",
    "3. It's useful for optimization algorithms that rely on gradients, such as gradient descent, as it provides smooth gradients.\n",
    "\n",
    "However, MSE is sensitive to outliers since squaring large errors can disproportionately affect the overall error. In scenarios where outliers are present or where a more robust metric is required, alternative metrics such as Mean Absolute Error (MAE) might be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714d484a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.768216Z",
     "iopub.status.busy": "2024-03-30T15:29:40.767112Z",
     "iopub.status.idle": "2024-03-30T15:29:40.773353Z",
     "shell.execute_reply": "2024-03-30T15:29:40.772183Z"
    },
    "papermill": {
     "duration": 0.015077,
     "end_time": "2024-03-30T15:29:40.775760",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.760683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_predicted):\n",
    "    # step 1, 2: Total difference\n",
    "    sum_diff = 0\n",
    "    for i in range(len(y_true)):\n",
    "        sum_diff += (y_predicted[i] - y_true[i]) ** 2\n",
    "    return sum_diff / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e90b12a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.788966Z",
     "iopub.status.busy": "2024-03-30T15:29:40.788472Z",
     "iopub.status.idle": "2024-03-30T15:29:40.794024Z",
     "shell.execute_reply": "2024-03-30T15:29:40.793131Z"
    },
    "papermill": {
     "duration": 0.014411,
     "end_time": "2024-03-30T15:29:40.796376",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.781965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error is: 0.366\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error is:\", mse(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243818f",
   "metadata": {
    "papermill": {
     "duration": 0.005354,
     "end_time": "2024-03-30T15:29:40.807036",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.801682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "__Same implementation in Numpy is:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bec8901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T15:29:40.819983Z",
     "iopub.status.busy": "2024-03-30T15:29:40.819195Z",
     "iopub.status.idle": "2024-03-30T15:29:40.825949Z",
     "shell.execute_reply": "2024-03-30T15:29:40.824788Z"
    },
    "papermill": {
     "duration": 0.015762,
     "end_time": "2024-03-30T15:29:40.828113",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.812351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.366"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.square(y_predicted - y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe50406",
   "metadata": {
    "papermill": {
     "duration": 0.005928,
     "end_time": "2024-03-30T15:29:40.839743",
     "exception": false,
     "start_time": "2024-03-30T15:29:40.833815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## There are more metrics\n",
    "In addition to Mean Squared Error (MSE) and Mean Absolute Error (MAE) and Log Loss, there are several other commonly used evaluation metrics in machine learning and regression tasks. Some of these metrics include:\n",
    "\n",
    "1. **Root Mean Squared Error (RMSE)**:\n",
    "   - RMSE is the square root of the Mean Squared Error. It's useful because it's in the same unit as the target variable, making it easier to interpret.\n",
    "   - RMSE = \\( \\sqrt{\\text{MSE}} \\)\n",
    "\n",
    "2. **Mean Absolute Percentage Error (MAPE)**:\n",
    "   - MAPE measures the average percentage difference between the predicted and actual values.\n",
    "   - MAPE is calculated as the mean of the absolute percentage errors:\n",
    "     $$ \\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\\% $$\n",
    "   - MAPE is useful when you want to evaluate the performance of a model relative to the scale of the target variable.\n",
    "\n",
    "3. **Coefficient of Determination (R-squared)**:\n",
    "   - R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "   - It ranges from 0 to 1, with higher values indicating a better fit of the model to the data.\n",
    "   - R-squared = $ 1 - \\frac{\\text{MSE}}{\\text{Var}(y)} $, where $Var(y)$ is the variance of the target variable.\n",
    "\n",
    "4. **Median Absolute Error (MedAE)**:\n",
    "   - MedAE is the median of the absolute errors between the predicted and actual values.\n",
    "   - It's less sensitive to outliers compared to mean-based metrics like MAE or MSE.\n",
    "\n",
    "5. **R-squared Adjusted (Adjusted R-squared)**:\n",
    "   - Adjusted R-squared is a modified version of R-squared that penalizes model complexity by taking into account the number of predictors in the model.\n",
    "   - It provides a more conservative estimate of the model's goodness of fit, particularly when dealing with multiple predictors.\n",
    "\n",
    "6. **Mean Squared Logarithmic Error (MSLE)**:\n",
    "   - MSLE measures the mean of the squared logarithmic differences between the predicted and actual values.\n",
    "   - It's useful when the target variable spans several orders of magnitude.\n",
    "\n",
    "These are just a few examples of evaluation metrics commonly used in regression tasks. The choice of metric depends on various factors, including the specific characteristics of the dataset, the problem domain, and the desired properties of the evaluation."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.590462,
   "end_time": "2024-03-30T15:29:41.265701",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-30T15:29:37.675239",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
