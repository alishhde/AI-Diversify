{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d8b54c",
   "metadata": {
    "papermill": {
     "duration": 0.006856,
     "end_time": "2024-05-05T07:27:29.127344",
     "exception": false,
     "start_time": "2024-05-05T07:27:29.120488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Word Embedding\n",
    "\n",
    "Word embedding is a technique used in natural language processing to map words or phrases *from a vocabulary* to *vectors of real numbers*, which represent the words in a continuous vector space. These embeddings capture semantic meanings, relationships, and context of the words, enabling algorithms to process text using these numerical representations.\n",
    "\n",
    "The primary benefits of word embeddings include:\n",
    "- **Dimensionality Reduction:** Instead of using high-dimensional one-hot encoded vectors, word embeddings reduce the dimensionality, which helps in handling the curse of dimensionality in large datasets.\n",
    "- **Semantic Similarity:** Words with similar meanings are often placed close together in the embedding space. For example, \"king\" and \"queen\" might be positioned nearer each other than \"king\" and \"apple\".\n",
    "- **Context Capture:** Modern embedding techniques take the context of words into account, which allows subtle differences in usage to be captured, such as the different meanings of \"bank\" in \"river bank\" and \"savings bank\".\n",
    "\n",
    "Some popular methods for generating word embeddings include:\n",
    "- **Word2Vec:** Developed by a team led by Tomas Mikolov at Google, it offers two architectures: Continuous Bag-of-Words (CBOW) and Skip-Gram. Both methods use a shallow neural network model to produce embeddings.\n",
    "- **GloVe (Global Vectors for Word Representation):** Developed by Stanford University researchers, GloVe constructs embeddings by analyzing word co-occurrence statistics across a corpus to learn relationships.\n",
    "- **FastText:** Developed by Facebook’s AI Research lab, FastText extends Word2Vec to consider subword information, which helps in capturing meanings of shorter words and enhances the understanding of morphologically rich languages.\n",
    "\n",
    "Word embeddings are a foundational technique in many NLP tasks like text classification, sentiment analysis, machine translation, and more. They can be trained from scratch or leveraged through pre-trained models available in libraries like TensorFlow, PyTorch, and Hugging Face’s Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e165080",
   "metadata": {
    "papermill": {
     "duration": 0.005705,
     "end_time": "2024-05-05T07:27:29.139536",
     "exception": false,
     "start_time": "2024-05-05T07:27:29.133831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Using Keras Embedding Layer for Word Embedding\n",
    "-----\n",
    "The Keras `Embedding` layer is a simple yet powerful tool in TensorFlow for handling word embeddings in deep learning models designed for natural language processing tasks. The layer essentially maps integer indices (which represent specific words) to dense vectors of fixed size. It's a way to convert text data into a form that neural networks can work with effectively.\n",
    "\n",
    "Here’s a basic breakdown of how the `Embedding` layer works and how to use it:\n",
    "\n",
    "### Key Features\n",
    "- **Input Dimension:** This is the size of the vocabulary, i.e., the total number of unique words in the dataset.\n",
    "- **Output Dimension:** This is the dimensionality of the embedding vector. Each word will be represented by a vector of this size.\n",
    "- **Input Length:** This is the length of input sequences, as all sequences need to be of the same length in a neural network.\n",
    "\n",
    "### Basic Usage\n",
    "When you instantiate an `Embedding` layer, you need to specify the `input_dim` (vocabulary size), `output_dim` (dimension of the embedding vector), and optionally, `input_length` (length of input sequences). The layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "\n",
    "Here is a simple example in Keras:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "# Adding Embedding layer\n",
    "# input_dim = 1000 (vocabulary size)\n",
    "# output_dim = 64 (embedding dimension)\n",
    "# input_length = 10 (length of input sequences)\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=10))\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### Training\n",
    "The `Embedding` layer is trained just like any other layer in a neural network: the model attempts to reduce the loss function during training, adjusting the weights (i.e., the embeddings) using backpropagation. Over time, these embeddings adjust to encapsulate useful properties and relationships among words based on the training data.\n",
    "\n",
    "### Benefits and Use Cases\n",
    "- **Reduced Dimensionality:** It maps high-dimensional one-hot vectors to lower-dimensional dense vectors.\n",
    "- **Context Sensitivity:** In combination with other layers (like LSTM or GRU), it can capture contextual relationships between words in sequences.\n",
    "- **Pre-trained Embeddings:** You can initialize the `Embedding` layer with pre-trained word embeddings such as Word2Vec or GloVe to enhance model performance, especially when you have limited data for training.\n",
    "\n",
    "### Initializing with Pre-trained Word Embeddings\n",
    "If you want to use pre-trained embeddings, you can load them and then initialize the `Embedding` layer with these weights. Here’s a brief example using GloVe embeddings:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Assume `embedding_matrix` is loaded from GloVe\n",
    "# input_dim is the vocabulary size\n",
    "# output_dim is the embedding dimension (e.g., 50, 100, 200, 300 for GloVe)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, \n",
    "                    output_dim=embedding_dimension, \n",
    "                    weights=[embedding_matrix], \n",
    "                    input_length=input_length, \n",
    "                    trainable=False))  # Set trainable to False to keep the embeddings fixed\n",
    "```\n",
    "\n",
    "This flexibility and simplicity make the Keras `Embedding` layer highly useful for various NLP tasks, such as sentiment analysis, text classification, and more, allowing for efficient learning and representation of text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244031a2",
   "metadata": {
    "papermill": {
     "duration": 0.005696,
     "end_time": "2024-05-05T07:27:29.151083",
     "exception": false,
     "start_time": "2024-05-05T07:27:29.145387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, let's see how it realy works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15bcdab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:29.164991Z",
     "iopub.status.busy": "2024-05-05T07:27:29.164339Z",
     "iopub.status.idle": "2024-05-05T07:27:44.655213Z",
     "shell.execute_reply": "2024-05-05T07:27:44.653924Z"
    },
    "papermill": {
     "duration": 15.501416,
     "end_time": "2024-05-05T07:27:44.658436",
     "exception": false,
     "start_time": "2024-05-05T07:27:29.157020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 07:27:31.364304: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-05 07:27:31.364446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-05 07:27:31.516807: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "reviews = ['nice food',\n",
    "        'amazing restaurant',\n",
    "        'too good',\n",
    "        'just loved it!',\n",
    "        'will go again',\n",
    "        'horrible food',\n",
    "        'never go there',\n",
    "        'poor service',\n",
    "        'poor quality',\n",
    "        'needs improvement']\n",
    "\n",
    "sentiment = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27287586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:44.689296Z",
     "iopub.status.busy": "2024-05-05T07:27:44.688590Z",
     "iopub.status.idle": "2024-05-05T07:27:44.696926Z",
     "shell.execute_reply": "2024-05-05T07:27:44.695816Z"
    },
    "papermill": {
     "duration": 0.02023,
     "end_time": "2024-05-05T07:27:44.699374",
     "exception": false,
     "start_time": "2024-05-05T07:27:44.679144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 29]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(\"amazing restaurant\", 30)  # `30` determines the range that the words can take a number and convert to it\n",
    "                                   # This number usually equals to the size of the vocabulary, making it possible \n",
    "                                   # for each word to have a unique value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e8057",
   "metadata": {
    "papermill": {
     "duration": 0.006084,
     "end_time": "2024-05-05T07:27:44.711811",
     "exception": false,
     "start_time": "2024-05-05T07:27:44.705727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Next we do the encoding on all the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c379b8f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:44.726712Z",
     "iopub.status.busy": "2024-05-05T07:27:44.726240Z",
     "iopub.status.idle": "2024-05-05T07:27:44.732813Z",
     "shell.execute_reply": "2024-05-05T07:27:44.731568Z"
    },
    "papermill": {
     "duration": 0.017862,
     "end_time": "2024-05-05T07:27:44.735989",
     "exception": false,
     "start_time": "2024-05-05T07:27:44.718127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15, 9], [5, 29], [26, 25], [26, 17, 23], [1, 1, 4], [8, 9], [11, 1, 9], [27, 23], [27, 27], [9, 29]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30\n",
    "encoded_reviews = [one_hot(review, vocab_size) for review in reviews]\n",
    "print(encoded_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17837a1",
   "metadata": {
    "papermill": {
     "duration": 0.006012,
     "end_time": "2024-05-05T07:27:44.748498",
     "exception": false,
     "start_time": "2024-05-05T07:27:44.742486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> We need to standardize the lengths of the sequences we achieved in the previous step. To do so, we used the `pad_sequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d1c52d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:44.763304Z",
     "iopub.status.busy": "2024-05-05T07:27:44.762880Z",
     "iopub.status.idle": "2024-05-05T07:27:44.769521Z",
     "shell.execute_reply": "2024-05-05T07:27:44.768305Z"
    },
    "papermill": {
     "duration": 0.017324,
     "end_time": "2024-05-05T07:27:44.772202",
     "exception": false,
     "start_time": "2024-05-05T07:27:44.754878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  9  0  0]\n",
      " [ 5 29  0  0]\n",
      " [26 25  0  0]\n",
      " [26 17 23  0]\n",
      " [ 1  1  4  0]\n",
      " [ 8  9  0  0]\n",
      " [11  1  9  0]\n",
      " [27 23  0  0]\n",
      " [27 27  0  0]\n",
      " [ 9 29  0  0]]\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "print(padded_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a4ad41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:44.786748Z",
     "iopub.status.busy": "2024-05-05T07:27:44.786359Z",
     "iopub.status.idle": "2024-05-05T07:27:44.859072Z",
     "shell.execute_reply": "2024-05-05T07:27:44.858088Z"
    },
    "papermill": {
     "duration": 0.083059,
     "end_time": "2024-05-05T07:27:44.861633",
     "exception": false,
     "start_time": "2024-05-05T07:27:44.778574",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeded_vector_size = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embeded_vector_size, name=\"embedding\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.build((None, max_length))  # Explicitly build the model and define the max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be59671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:44.876808Z",
     "iopub.status.busy": "2024-05-05T07:27:44.875835Z",
     "iopub.status.idle": "2024-05-05T07:27:44.880906Z",
     "shell.execute_reply": "2024-05-05T07:27:44.879748Z"
    },
    "papermill": {
     "duration": 0.015627,
     "end_time": "2024-05-05T07:27:44.883652",
     "exception": false,
     "start_time": "2024-05-05T07:27:44.868025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = padded_reviews\n",
    "y = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88135a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:44.898190Z",
     "iopub.status.busy": "2024-05-05T07:27:44.897783Z",
     "iopub.status.idle": "2024-05-05T07:27:44.929858Z",
     "shell.execute_reply": "2024-05-05T07:27:44.928693Z"
    },
    "papermill": {
     "duration": 0.042242,
     "end_time": "2024-05-05T07:27:44.932384",
     "exception": false,
     "start_time": "2024-05-05T07:27:44.890142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m5\u001b[0m)           │           \u001b[38;5;34m150\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171</span> (684.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m171\u001b[0m (684.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171</span> (684.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m171\u001b[0m (684.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a44d9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:44.949614Z",
     "iopub.status.busy": "2024-05-05T07:27:44.949169Z",
     "iopub.status.idle": "2024-05-05T07:27:48.656663Z",
     "shell.execute_reply": "2024-05-05T07:27:48.655265Z"
    },
    "papermill": {
     "duration": 3.719965,
     "end_time": "2024-05-05T07:27:48.659485",
     "exception": false,
     "start_time": "2024-05-05T07:27:44.939520",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7a0d1091a950>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=80, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c14588f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:48.677262Z",
     "iopub.status.busy": "2024-05-05T07:27:48.676872Z",
     "iopub.status.idle": "2024-05-05T07:27:48.909544Z",
     "shell.execute_reply": "2024-05-05T07:27:48.908402Z"
    },
    "papermill": {
     "duration": 0.244864,
     "end_time": "2024-05-05T07:27:48.912077",
     "exception": false,
     "start_time": "2024-05-05T07:27:48.667213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.5614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f08e6",
   "metadata": {
    "papermill": {
     "duration": 0.007718,
     "end_time": "2024-05-05T07:27:48.927777",
     "exception": false,
     "start_time": "2024-05-05T07:27:48.920059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, the actual `word embeddings` are the acquired weights in embedding layer.  Which are accessible from the following command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0757bb1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T07:27:48.945768Z",
     "iopub.status.busy": "2024-05-05T07:27:48.945001Z",
     "iopub.status.idle": "2024-05-05T07:27:48.955966Z",
     "shell.execute_reply": "2024-05-05T07:27:48.954668Z"
    },
    "papermill": {
     "duration": 0.023357,
     "end_time": "2024-05-05T07:27:48.958956",
     "exception": false,
     "start_time": "2024-05-05T07:27:48.935599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of weights is:  30\n",
      "[[ 0.0191403  -0.01421062  0.0062328  -0.05365282 -0.0060914 ]\n",
      " [ 0.04781847 -0.09340276 -0.04859197 -0.08498313 -0.03928798]\n",
      " [ 0.01348182  0.02770411  0.01283503  0.01935459 -0.0065825 ]\n",
      " [-0.03045956  0.02385424 -0.0367759  -0.03711759 -0.00880497]\n",
      " [ 0.10512549 -0.06518326 -0.05221771  0.10601646  0.07076833]]\n"
     ]
    }
   ],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "print(\"The number of weights is: \", len(weights))\n",
    "print(weights[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d9b90",
   "metadata": {
    "papermill": {
     "duration": 0.007687,
     "end_time": "2024-05-05T07:27:48.974786",
     "exception": false,
     "start_time": "2024-05-05T07:27:48.967099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each word is embedded into a vector of 5 values as we determined this. "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24.533981,
   "end_time": "2024-05-05T07:27:50.508868",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-05T07:27:25.974887",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
